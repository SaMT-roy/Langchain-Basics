{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41294567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import google.generativeai as palm\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.vectorstores import chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ae2bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] =  ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2f99f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"buildings-12-01787-v3.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "594b83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GooglePalm(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9e8c26cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep forest-DQN is a deep reinforcement learning method that uses a deep forest classifier to reduce\n",
      "the action space and speed up convergence. DF-DQN has a good energy-saving effect in engineering\n",
      "applications but is not suitable for systems lacking historical data or expert experience.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "\n",
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"map_reduce\")\n",
    "\n",
    "\n",
    "output_summary = chain.run(pages)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65beefeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- DF-DQN is a deep reinforcement learning method for energy-saving control of cooling water system\n",
      "in HVAC.\n",
      "- DF-DQN uses a deep forest classifier to map the action space to a smaller one.\n",
      "- DF-DQN\n",
      "converges much faster than DQN.\n",
      "- DF-DQN performs slightly worse than the model-based control method\n",
      "in saving energy, but it does not require any complete system model.\n",
      "- DF-DQN has a good energy-\n",
      "saving effect in engineering applications, but it is not suitable for systems lacking historical\n",
      "data or expert experience.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, \n",
    "                        input_variables=[\"text\"])\n",
    "\n",
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"map_reduce\",\n",
    "                             map_prompt=BULLET_POINT_PROMPT, \n",
    "                             combine_prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "# chain.llm_chain.prompt= BULLET_POINT_PROMPT\n",
    "# chain.combine_document_chain.llm_chain.prompt= BULLET_POINT_PROMPT\n",
    "\n",
    "output_summary = chain.run(pages)\n",
    "wrapped_text = textwrap.fill(output_summary, \n",
    "                             width=100,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c13bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b5cace4",
   "metadata": {},
   "source": [
    "# chat with pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc85e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9183c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_reader = PdfReader('impromptu-rh.pdf')\n",
    "\n",
    "raw_text = ''\n",
    "for i, page in enumerate(doc_reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text\n",
    "        \n",
    "# Splitting up the text into smaller chunks for indexing\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200, #striding over the text\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1cd5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GooglePalmEmbeddings()\n",
    "docsearch = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0304452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how does GPT-4 change social media?\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f33b3bb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when I started thinking about GPT-4’s potential impact on \n",
      "social media was to spend an hour or so feeding GPT-4 various \n",
      "prompts to generate titles for potential MrBeast videos. While \n",
      "GPT-4 produced fairly generic responses much of the time, its 106Impromptu: Amplifying Our Humanity Through AI\n",
      "extreme generativity means its success rate doesn’t have to be \n",
      "high. Here’s a few that I felt were its best responses:\n",
      "Reid: Create twenty-five titles for funny MrBeast videos \n",
      "that feature a Lamborghini versus something else.\n",
      "GPT-4:  Lamborghini vs 10,000 Balloons: Can It Float?\n",
      "Reid: Create twenty-five titles for hilarious and creative \n",
      "MrBeast videos involving hot sauce.\n",
      "GPT-4:  I Filled a Pool with Hot Sauce and Dared My \n",
      "Friends to Swim in It\n",
      "Reid: Create the headline for a MrBeast video if MrBeast \n",
      "were a radical Marxist.\n",
      "GPT-4:  I Bought an Entire Factory and Gave It to \n",
      "the Workers\n",
      "Reid: Create twenty titles for wacky and inventive \n",
      "MrBeast videos that involve the world’s largest 3D\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f591c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "677be59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "chain = load_qa_chain(GooglePalm(), \n",
    "                      chain_type=\"stuff\") # we are going to stuff all the docs in at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57f8b150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09a802f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reid Hoffman, Sam Altman'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"who are the authors of the book?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b43228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saptarshimallikthakur/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intermediate_steps': [{'answer': 'This document does not answer the question',\n",
       "   'score': '0'},\n",
       "  {'answer': 'a company that developed a chatbot', 'score': '50'},\n",
       "  {'answer': 'research organization founded in 2015 with a mission to give millions of people direct, hands-on access to powerful new AI tools',\n",
       "   'score': '80'},\n",
       "  {'answer': 'an organization founded in 2015 to develop technologies that put the power of AI directly into the hands of millions of people',\n",
       "   'score': '80'},\n",
       "  {'answer': 'This document does not answer the question', 'score': '0'},\n",
       "  {'answer': 'This document does not answer the question', 'score': '0'},\n",
       "  {'answer': 'This document does not answer the question', 'score': '0'},\n",
       "  {'answer': 'gence tools for the benefit of humanity', 'score': '85'},\n",
       "  {'answer': 'non-profit AI research company', 'score': '80'},\n",
       "  {'answer': 'a non-profit research organization founded in 2015 with a mission to give millions of people direct, hands-on access to powerful new AI tools.',\n",
       "   'score': '100'}],\n",
       " 'output_text': 'a non-profit research organization founded in 2015 with a mission to give millions of people direct, hands-on access to powerful new AI tools.'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_chain(GooglePalm(), \n",
    "                      chain_type=\"map_rerank\",\n",
    "                      return_intermediate_steps=True\n",
    "                      ) \n",
    "\n",
    "query = \"who are openai?\"\n",
    "docs = docsearch.similarity_search(query,k=10)\n",
    "results = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fec78e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a non-profit research organization founded in 2015 with a mission to give millions of people direct, hands-on access to powerful new AI tools.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b80846c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nIn addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\\n\\nQuestion: [question here]\\nHelpful Answer: [answer here]\\nScore: [score between 0 and 100]\\n\\nHow to determine the score:\\n- Higher is a better answer\\n- Better responds fully to the asked question, with sufficient level of detail\\n- If you do not know the answer based on the context, that should be a score of 0\\n- Don't be overconfident!\\n\\nExample #1\\n\\nContext:\\n---------\\nApples are red\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: red\\nScore: 100\\n\\nExample #2\\n\\nContext:\\n---------\\nit was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\\n---------\\nQuestion: what type was the car?\\nHelpful Answer: a sports car or an suv\\nScore: 60\\n\\nExample #3\\n\\nContext:\\n---------\\nPears are either red or orange\\n---------\\nQuestion: what color are apples?\\nHelpful Answer: This document does not answer the question\\nScore: 0\\n\\nBegin!\\n\\nContext:\\n---------\\n{context}\\n---------\\nQuestion: {question}\\nHelpful Answer:\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the prompt\n",
    "chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026208d",
   "metadata": {},
   "source": [
    "# RetrievalQA\n",
    "RetrievalQA chain uses load_qa_chain and combines it with the a retriever (in our case the FAISS index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d49e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# set up FAISS as a generic retriever \n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
    "\n",
    "# create the chain to answer questions \n",
    "rqa = RetrievalQA.from_chain_type(llm=GooglePalm(), \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8eb7902d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is OpenAI?',\n",
       " 'result': 'OpenAI is a research organization founded in 2015 with a mission to give millions of people direct, hands-on access to powerful new AI tools.',\n",
       " 'source_documents': [Document(page_content='Stable Diffusion, a new kind of opt-in, user-driven, and very \\nvisible AI usage suddenly exists. Users share their outputs, \\ntechniques, experiences, and opinions on Twitter, YouTube, \\nGithub, Discord, and more. Diverse viewpoints from around \\nthe world, informed by hands-on usage, shape this discourse, \\nwhich is always spirited, often fractious, and, to my mind, \\nhighly productive.\\nMillions of people, including many whose main goal is to find \\nflaws in these systems, are getting a shot to shape the further \\nevolution of AI through their usage, feedback, and critiques. As 233Conclusion: At the Crossroads of the 21st Century\\nOpenAI co-founder and CEO Sam Altman exclaimed in a recent \\npost on OpenAI’s website, “We currently believe the best way to \\nsuccessfully navigate AI deployment challenges is with a tight \\nfeedback loop of rapid learning and careful iteration.”\\nIn other words, the approach OpenAI and other AI developers \\nare now employing exists as a healthy and more democratic'),\n",
       "  Document(page_content='sion from Congress.\\nWanting to protect society from bad tech outcomes is not a new \\nphenomenon, of course. In fact, it’s exactly this sentiment that \\nled OpenAI’s founders to create their organization in 2015.\\nSo what’s the most effective and inclusive way to achieve good \\noutcomes for society in the long term?\\nIn recent years, the predominant critique of AI is that it is some-\\nthing that has largely been happening to individuals rather \\nthan for them—an under-the-radar force deployed by Big Tech \\nwithout much public knowledge, much less consent, via tech -\\nnologies like facial recognition and algorithmic decision-mak-\\ning on home loans, job applicant screening, social media rec-\\nommendations, and more.\\nA founding goal of OpenAI was to develop technologies that put \\nthe power of AI directly into the hands of millions of people. \\nIn this way, AI might function as a decentralized, personally \\nempowering force, rather than a top-down, totalizing one.'),\n",
       "  Document(page_content='zation founded in 2015 with a mission to give millions of people \\ndirect, hands-on access to powerful new AI tools.2Impromptu: Amplifying Our Humanity Through AI\\nAs one of OpenAI’s original funders, I’ve been experimenting \\nwith its products for some time now, so my query to GPT-4 was \\nnot the first time I’d asked an LLM to create a lightbulb joke \\nfor me. (I’m fond of lightbulb jokes.) While earlier versions of \\nGPT sometimes understood the assignment—especially if the \\njoke formula’s main variable is part of the lightbulb-joke canon, \\nlike “lawyer” or “psychologist”—their outputs have tended to \\nbe merely OK. If fed less conventional setups, earlier GPTs can \\nfreeze up like an amateur at an open-mic night:\\nReid: How many restaurant inspectors does it take to \\nchange a light bulb? \\nGPT-3: Only one, but the light bulb has to want \\nto change.\\nThat robot should keep its day job.\\nCompare that with the response I got when I submitted the \\nsame prompt to GPT-4:'),\n",
       "  Document(page_content='GPT-4, “How many restaurant inspectors does it take to change \\na lightbulb?” \\nGPT-4, as you may know, is an advanced type of AI system, or \\nnatural-language processor, known as a large language model \\n(LLM). Prompt it with a few words or sentences and it will \\ngenerate coherent and diverse texts in response. In this way, it \\ncan answer questions, perform tasks, and productively interact \\nwith its human users.\\nAlong with its predecessors, including the wildly popular \\nChatGPT, GPT-4 was developed by OpenAI, a research organi-\\nzation founded in 2015 with a mission to give millions of people \\ndirect, hands-on access to powerful new AI tools.\\n1INTRODUCTION: MOMENTS OF \\nENLIGHTENMENT\\nFor Isaac Newton, it was an apple falling from a tree that \\nsupposedly led him to formulate the law of universal gravity. \\nFor Benjamin Franklin, it was a key on a kite string, struck by \\nlightning in a thunderstorm, that proved electricity could be \\ntransferred and stored.')]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rqa(\"What is OpenAI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa0954af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The last 20 years have been mostly bad news for the American journalism industry.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what have the last 20 years been like for American journalism?\"\n",
    "rqa(query)['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd4528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
