{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aa51f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI, OpenAIChat\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "020dd67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"buildings-12-01787-v3.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6509ddb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This paper reviews the use of deep reinforcement learning for autonomous control of water heaters\n",
      "and HVAC systems in buildings. It compares the energy-saving performance of four different control\n",
      "strategies for HVAC systems in buildings, and finds that Model-Based Control has the highest energy\n",
      "saving performance, followed by DQN and DF-DQN. It also reviews the literature on reinforcement\n",
      "learning for building energy efficiency control, building energy consumption prediction, and\n",
      "stochastic chiller sequencing control.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "\n",
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"map_reduce\")\n",
    "\n",
    "\n",
    "output_summary = chain.run(pages)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1360499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Citation: Han, Z.; Fu, Q.; Chen, J.;\n",
      "Wang, Y.; Lu, Y.; Wu, H.; Gui, H.\n",
      "Deep Forest-Based DQN for Cooling\n",
      "Water System Energy Saving Control\n",
      "in HVAC. Buildings 2022 ,12, 1787.\n",
      "https://doi.org/10.3390/\n",
      "buildings12111787\n",
      "Academic Editors: Shi-Jie Cao,\n",
      "Dahai Qi, Junqi Wang and\n",
      "Gwanggil Jeon\n",
      "Received: 21 August 2022\n",
      "Accepted: 17 October 2022\n",
      "Published: 25 October 2022\n",
      "Publisher’s Note: MDPI stays neutral\n",
      "with regard to jurisdictional claims in\n",
      "published maps and institutional afﬁl-\n",
      "iations.\n",
      "Copyright: © 2022 by the authors.\n",
      "Licensee MDPI, Basel, Switzerland.\n",
      "This article is an open access article\n",
      "distributed under the terms and\n",
      "conditions of the Creative Commons\n",
      "Attribution (CC BY) license (https://\n",
      "creativecommons.org/licenses/by/\n",
      "4.0/).\n",
      "buildings\n",
      "Article\n",
      "Deep Forest-Based DQN for Cooling Water System Energy\n",
      "Saving Control in HV AC\n",
      "Zhicong Han1,2, Qiming Fu1,2,*, Jianping Chen2,3,*, Yunzhe Wang1,2, You Lu1,2\n",
      ", Hongjie Wu1\n",
      "and Hongguan Gui4\n",
      "1School of Electronic and Information Engineering, Suzhou University of Science and Technology,\n",
      "Suzhou 215009, China\n",
      "2Jiangsu Province Key Laboratory of Intelligent Building Energy Efﬁciency,\n",
      "Suzhou University of Science and Technology, Suzhou 215009, China\n",
      "3School of Architecture and Urban Planning, Suzhou University of Science and Technology,\n",
      "Suzhou 215009, China\n",
      "4Data Grand Information Technology, Co., Ltd., Shanghai 200120, China\n",
      "*Correspondence: fqm_1@mail.usts.edu.cn (Q.F.); alanjpchen@aliyun.com (J.C.)\n",
      "Abstract: Currently, reinforcement learning (RL) has shown great potential in energy saving in HVAC\n",
      "systems. However, in most cases, RL takes a relatively long period to explore the environment\n",
      "before obtaining an excellent control policy, which may lead to an increase in cost. To reduce the\n",
      "unnecessary waste caused by RL methods in exploration, we extended the deep forest-based deep\n",
      "Q-network (DF-DQN) from the prediction problem to the control problem, optimizing the running\n",
      "frequency of the cooling water pump and cooling tower in the cooling water system. In DF-DQN,\n",
      "it uses the historical data or expert experience as a priori knowledge to train a deep forest (DF)\n",
      "classiﬁer, and then combines the output of DQN to attain the control frequency, where DF can map\n",
      "the original action space of DQN to a smaller one, so DF-DQN converges faster and has a better\n",
      "energy-saving effect than DQN in the early stage. In order to verify the performance of DF-DQN, we\n",
      "constructed a cooling water system model based on historical data. The experimental results show\n",
      "that DF-DQN can realize energy savings from the ﬁrst year, while DQN realized savings from the\n",
      "third year. DF-DQN’s energy-saving effect is much better than DQN in the early stage, and it also has\n",
      "a good performance in the latter stage. In 20 years, DF-DQN can improve the energy-saving effect by\n",
      "11.035% on average every year, DQN can improve by 7.972%, and the model-based control method\n",
      "can improve by 13.755%. Compared with traditional RL methods, DF-DQN can avoid unnecessary\n",
      "waste caused by exploration in the early stage and has a good performance in general, which indicates\n",
      "that DF-DQN is more suitable for engineering practice.\n",
      "Keywords: HVAC; cooling water system; reinforcement learning; DF-DQN\n",
      "1. Introduction\n",
      "In order to achieve the goal of carbon neutrality, countries around the world are com-\n",
      "mitted to energy saving and emission reduction. Building energy consumption accounts\n",
      "for a large part of energy consumption around the world [ 1], and heating, ventilation, and\n",
      "air-conditioning (HVAC) systems occupy a major part, reaching more than half of energy\n",
      "consumption. The cooling water system is an essential subsystem of the HVAC system,\n",
      "which mainly consists of cooling water pumps, cooling towers, and chiller condensers [ 2].\n",
      "The operation of the cooling water system has an important inﬂuence on the entire HVAC\n",
      "system, and optimal control of the cooling water system can effectively reduce energy\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The operation of the cooling water system has an important inﬂuence on the entire HVAC\n",
      "system, and optimal control of the cooling water system can effectively reduce energy\n",
      "consumption of the HVAC system. Thus, the optimal control of the cooling water system\n",
      "is crucial.\n",
      "In HVAC systems, optimal control policies are often used to reduce operation costs and\n",
      "to ensure the thermal comfort of occupants [ 3,4]. Optimal control policies can be classiﬁed\n",
      "Buildings 2022 ,12, 1787. https://doi.org/10.3390/buildings12111787 https://www.mdpi.com/journal/buildings\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 2 of 22\n",
      "into traditional control policies and advanced control policies in intelligent buildings, where\n",
      "the former one contains sequencing control (rule-based control) and process control, and\n",
      "the latter one includes soft-computing control policies, hard-computing control policies,\n",
      "and hybrid control policies [ 5]. Many optimal control methods have been tried for cooling\n",
      "water system control, such as proportional-integral (PI) controllers, proportional integral\n",
      "derivative (PID) controllers, and model predictive control (MPC) controllers. These meth-\n",
      "ods heavily rely on the system model, various sensors, and controllers in the system, so the\n",
      "disadvantages of these methods are also obvious. Model-based methods often require a\n",
      "perfect model of the system, while system modeling is usually difﬁcult in real applications\n",
      "even if we can attain enough data from different sensors. According to Zhu et al., the\n",
      "uncertainties of the model have a serious impact on the control performance [ 6]. In the\n",
      "actual system operation, the aging of the equipment or the renewal of some equipment\n",
      "may lead to inconsistency between the system model and the actual system [ 7]. Even if the\n",
      "initially established model is accurate enough, continuous changes in the actual system\n",
      "over time lead to an unavoidable decrease in the performance of the control method.\n",
      "To avoid the impact of the imperfect system model on control policies, data-driving\n",
      "methods in artiﬁcial intelligence have received too much attention in HVAC control prob-\n",
      "lems recently. Reinforcement learning (RL) is a kind of classical data-driven and model-free\n",
      "method in artiﬁcial intelligence. In recent years, RL has attracted increasing attention\n",
      "for building energy efﬁciency control problems [ 8,9], because it can provide a simple\n",
      "framework by learning from interaction with the environment directly. In these studies,\n",
      "RL methods can provide a model-free framework for achieving energy saving, but they\n",
      "often fail to achieve a good control effect in the early stage, or can be even worse than\n",
      "some baseline control policies, which are mainly caused by the agent’s exploration of the\n",
      "environment. Moreover, in the exploration process by the trial-and-error mechanism, they\n",
      "may also cause a certain degree of damage to the equipment, which may directly lead to an\n",
      "increase in cost. These two problems severely limit the practical use of RL in the ﬁeld of\n",
      "HVAC optimization applications. Therefore, in order to maintain RL control effectiveness\n",
      "and achieve the maximum possible energy savings, it is necessary to reduce the time of\n",
      "this process in some way so that the RL control policy converges more quickly to reduce\n",
      "unnecessary costs.\n",
      "In this paper, we tried to use DF-DQN to tackle this problem. Due to the introduction\n",
      "of DF, we mapped the original action space to a smaller one, and then combined the label\n",
      "of DF to attain the ﬁnal control action, which directly reduced the output action of DQN.\n",
      "Moreover, the label of DF had the guidance of a priori knowledge, which not only ensured\n",
      "a good control effect, but also can realize energy saving in the early stage. The main\n",
      "contributions of this paper are as follows:\n",
      "• We extended our previously proposed DF-DQN from the prediction problem to the\n",
      "control problem. The introduction of DF mapped the original DQN output action\n",
      "space into a new smaller action space, which could accelerate the convergence speed\n",
      "of DQN;\n",
      "• We used DF-DQN to control a cooling water system in HVAC and to realize energy\n",
      "savings from the early stage. A priori knowledge was introduced as a deep forest\n",
      "classiﬁer, which can not only reduce the action space, but also reduce the exploration\n",
      "of the agent. The experimental results show that DF-DQN can save energy from the\n",
      "ﬁrst year, while DQN can achieve similar energy saving from the third year;\n",
      "• We veriﬁed the performance of DF-DQN in an environment based on the modeling\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"ﬁrst year, while DQN can achieve similar energy saving from the third year;\n",
      "• We veriﬁed the performance of DF-DQN in an environment based on the modeling\n",
      "of a real cooling water system, so as to ensure the credibility of DF-DQN. The data\n",
      "that DF-DQN and other compared methods used were collected from a real-world\n",
      "system, and the simulation environment was built based on this system. The code and\n",
      "the experimental data are available at: https://github.com/H-Phoebe/DF-DQN-for-\n",
      "energy-saving-control (accessed on 20 August 2022).\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 3 of 22\n",
      "2. Related Works\n",
      "In recent years, more and more researchers have tried to solve practical problems with\n",
      "RL methods. In the applications of the HVAC system, the complexity and lag of the HVAC\n",
      "system directly lead to an increase in modeling cost, while RL can provide model-free\n",
      "control and have good control performance. However, RL generally takes a relatively long\n",
      "time to learn a better control policy, and this process may lead to some unnecessary energy\n",
      "wastage and cost increases, so some researchers try to avoid this wastage by speeding up\n",
      "the convergence of RL algorithms, which can achieve more energy saving at the same time.\n",
      "Applications of RL in HV AC. Lork et al. [ 10] used Q-learning to achieve a balance\n",
      "between comfort and energy savings in rooms. They used a Bayesian convolutional neural\n",
      "network combined with data from all rooms to construct a temperature and air conditioning\n",
      "power prediction model to reduce uncertainty. This model was then adapted to individual\n",
      "rooms and the temperature set point was controlled using Q-learning. Qiu et al. [ 8] used\n",
      "Q-learning to obtain optimal control of the cooling water system in HVAC, wherein the RL\n",
      "controller can save 11% of the system energy, more than the 7% saved by the local feedback\n",
      "controller. Ahn et al. [ 11] used DQN to achieve a model-free optimal control policy in\n",
      "HVAC and the results proved that DQN can reduce energy consumption, and provided\n",
      "model-free optimal control. Brandi et al. [ 12] used DQN to control the water supply\n",
      "temperature set point of the heating system terminal unit, which can achieve a heating\n",
      "energy saving ranging between 5% and 12%. Yan et al. [ 13] applied DDPG to generate\n",
      "an optimal control policy for a multi-zone residential HVAC system, which can greatly\n",
      "reduce energy consumption while ensuring comfort. In addition, the DDPG-trained agent\n",
      "can intelligently balance different optimization objectives with generalization ability and\n",
      "adaptability to unknown environments. Ding et al. [ 14] used RL algorithms to control the\n",
      "indoor temperature of a residential HVAC system, which can achieve energy conservation\n",
      "while maintaining indoor thermal comfort. Qiu et al. [ 15] used three multi-agent RL\n",
      "algorithms to control the condenser system in HVAC. The experimental results showed that\n",
      "the interaction multi-agent RL algorithm can achieve better energy-saving effects compared\n",
      "to the other two algorithms. Amasyali et al. [ 16] used the deep RL controller to control\n",
      "the power cost of electric water heaters in residential buildings. The experimental results\n",
      "showed that this method does not cause discomfort to users, and can save 19–35% of the\n",
      "power cost compared with the baseline control.\n",
      "Improve RL convergence speed. In engineering applications, the convergence time\n",
      "of RL methods may be several months or even years, which directly leads to an increase\n",
      "in cost. Therefore, some researchers have tried to shorten this time to reduce the cost of\n",
      "practical applications. Li et al. [ 17] controlled the HVAC system in order to control energy\n",
      "consumption and ensure comfort, and they put forward multi-grid Q-learning to solve the\n",
      "problem of slow convergence rate in RL. Yu [ 18] et al. developed an exploration policy for\n",
      "the RL controller using a priori knowledge, which can guide the RL controller to explore\n",
      "the action space, thus reducing the training time. Fu et al. [ 19] used a multi-agent RL to\n",
      "realize the collaborative control optimization of multiple devices in the HVAC system.\n",
      "The experimental results showed that the method converges faster than single-agent RL\n",
      "method. In [ 20], the authors mention that adding a priori knowledge can help the RL\n",
      "controller reduce training time.\n",
      "3. Preliminaries\n",
      "3.1. MDP\n",
      "RL, as a class of control techniques in machine learning, has been explored for its\n",
      "potential in HVAC systems. In RL, the problem can often be considered as a sequential\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3. Preliminaries\n",
      "3.1. MDP\n",
      "RL, as a class of control techniques in machine learning, has been explored for its\n",
      "potential in HVAC systems. In RL, the problem can often be considered as a sequential\n",
      "decision-making case, and the agent can learn by interacting with the environment directly,\n",
      "as shown in Figure 1.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 4 of 22\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW  4 of 22  \n",
      "decision -making case, and the agent can learn by interacting with the environment di-\n",
      "rectly, as show n in Figure 1.  \n",
      " \n",
      "Figure 1. The interaction progress between agent s and environment s in RL.  \n",
      "Markov’s decision process (MDP), a classical formalization of sequential decision -\n",
      "making, is often used to model RL problems. An MDP can be defined as a tuple <\n",
      "𝑆,𝐴,𝑇,𝑅,𝛾>, where 𝑆 is the collection of states, 𝐴 is the collection of actions, 𝑇 is a tran-\n",
      "sition function, 𝑅 is an immediate reward function,  and 𝛾 is a discount factor. In the in-\n",
      "teractive process, for some state s, the agent can select an action to act on the environment, \n",
      "and receive a scalar reward and a next state [21]. The final goal of RL is to maximize a \n",
      "cumulative numerical reward, 𝑅𝑡, as is shown in Equation (1). \n",
      "𝑅𝑡= ∑𝛾𝑘𝑟𝑡+𝑘+1∞\n",
      "𝑘=0 (1) \n",
      "wher e 𝛾∈[0,1] is the discount factor, 𝑘 represents 𝑘 time steps after time step t, and \n",
      "𝑟𝑡+𝑘+1 represents the immediate reward of the corresponding time step. The agent selects \n",
      "an action 𝑎 𝜖 𝐴 by policy 𝜋, then the agent moves to the next state 𝑠𝑡+1, the n the agent \n",
      "obtains the immediate reward 𝑟𝑡+1 from the environment. In RL, the action value 𝑄 is \n",
      "used to represent the exception of a cumulative discounted reward, which is starting from \n",
      "state 𝑠 and taking action 𝑎. The action value 𝑄 is shown in Equation (2). \n",
      "𝑄𝜋(𝑠,𝑎)=𝐸𝜋[∑𝛾𝑘∞\n",
      "𝑘=0𝑅𝑡+𝑘+1|𝑠𝑡=𝑠,𝑎𝑡=𝑎] (2) \n",
      "The optimal policy 𝜋∗ can be achieved by evaluating the action value function:  \n",
      "𝑄∗(𝑠,𝑎)=max 𝑄𝜋(𝑠,𝑎)=𝐸[𝑅𝑡+1+𝛾 max\n",
      "𝑎′𝑄∗(𝑠𝑡+1,𝑎′)|𝑠𝑡=𝑠,𝑎𝑡=𝑎] (3) \n",
      "Finally, the optimal policy 𝜋∗ can be obtain ed. \n",
      "3.2. Deep Forest  \n",
      "Deep  forest  (DF) [22] is a decision tree ensemble approach and can be applied to clas-\n",
      "sification tasks. DF can obtain good performance in most cases, even with different data \n",
      "in different domains, which mainly benefits fro m two techniques, namely multi -grained \n",
      "scanning and  the cascade forest structure.  \n",
      "Multi -grained scanning uses sliding windows of various sizes for sampling to obtain \n",
      "more feature sub -samples, so as to obtain more and richer feature relationships. Then, a \n",
      "certain amount of  the random forest and cascade  forest are trained with the obtained fea-\n",
      "ture sub -samples to obtain the feature vector.  \n",
      "Figure 1. The interaction progress between agents and environments in RL.\n",
      "Markov’s decision process (MDP), a classical formalization of sequential decision-making,\n",
      "is often used to model RL problems. An MDP can be defined as a tuple ⟨S,A,T,R,γ⟩,where\n",
      "Sis the collection of states, Ais the collection of actions, Tis a transition function, Ris an\n",
      "immediate reward function, and γis a discount factor. In the interactive process, for some\n",
      "states, the agent can select an action to act on the environment, and receive a scalar reward\n",
      "and a next state [ 21]. The ﬁnal goal of RL is to maximize a cumulative numerical reward,\n",
      "Rt, as is shown in Equation (1).\n",
      "Rt=∞\n",
      "∑\n",
      "k=0γkrt+k+1 (1)\n",
      "where γ∈[0, 1]is the discount factor, krepresents ktime steps after time step t, and rt+k+1\n",
      "represents the immediate reward of the corresponding time step. The agent selects an\n",
      "action aϵAby policy π, then the agent moves to the next state st+1, then the agent obtains\n",
      "the immediate reward rt+1from the environment. In RL, the action value Qis used to\n",
      "represent the exception of a cumulative discounted reward, which is starting from state s\n",
      "and taking action a. The action value Qis shown in Equation (2).\n",
      "Qπ(s,a)=Eπ[\n",
      "∞\n",
      "∑\n",
      "k=0γkRt+k+1|st=s,at=a]\n",
      "(2)\n",
      "The optimal policy π∗can be achieved by evaluating the action value function:\n",
      "Q∗(s,a)=maxQπ(s,a)=E[\n",
      "Rt+1+γmax\n",
      "a′Q∗(\n",
      "st+1,a′)|st=s,at=a]\n",
      "(3)\n",
      "Finally, the optimal policy π∗can be obtained.\n",
      "3.2. Deep Forest\n",
      "Deep forest (DF) [ 22] is a decision tree ensemble approach and can be applied to\n",
      "classiﬁcation tasks. DF can obtain good performance in most cases, even with different data\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"3.2. Deep Forest\n",
      "Deep forest (DF) [ 22] is a decision tree ensemble approach and can be applied to\n",
      "classiﬁcation tasks. DF can obtain good performance in most cases, even with different data\n",
      "in different domains, which mainly beneﬁts from two techniques, namely multi-grained\n",
      "scanning and the cascade forest structure.\n",
      "Multi-grained scanning uses sliding windows of various sizes for sampling to obtain\n",
      "more feature sub-samples, so as to obtain more and richer feature relationships. Then,\n",
      "a certain amount of the random forest and cascade forest are trained with the obtained\n",
      "feature sub-samples to obtain the feature vector.\n",
      "The cascade forest structure is used to enhance the representation learning ability\n",
      "of DF. In a cascade forest, each level receives the characteristic information processed\n",
      "by the previous level, then the processing results in inputs, which are then output to\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 5 of 22\n",
      "the next level. The ﬁrst level’s input of a cascade forest is the feature vector after multi-\n",
      "granularity scanning transformation. The ﬁnal prediction result is obtained at the last level\n",
      "and expressed as an aggregate value.\n",
      "In addition, the training process of the deep forest is efﬁcient, and it can operate\n",
      "normally even if the training data scale is small. The structure of DF is shown in Figure 2.\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW  5 of 22  \n",
      "The c ascade forest structure is used to enhance the representation learning ability of \n",
      "DF. In a cascade forest, each level  receives the characteristic information processed by the \n",
      "previous level, then the  processing results in input s, which are then output  to the next \n",
      "level. The first level’s input of a cascade forest is the feature vector after multi -granularity \n",
      "scanning tra nsformation. The final prediction result is obtained at the last level and ex-\n",
      "pressed as an aggregate value.  \n",
      "In addition, the training process of the deep forest is efficient, and it can operate nor-\n",
      "mally even if the training data scale is small. T he structu re of DF is shown in Figure 2.  \n",
      " \n",
      "Figure 2. Structure of deep forest.  \n",
      "3.3. DQN  \n",
      "Traditional methods in RL, such as SARSA and Q -learning, can effectively solve \n",
      "problems with  a small state and action space by establishing Q -table. However, when the \n",
      "state space is large enough or continuous, such as practical problems in HVAC, these \n",
      "methods may fail to achieve a control policy. DQN, a method proposed by Google ’s Deep-\n",
      "Mind in 2015 [23], has been applied in HVAC control s in recent years. Different from \n",
      "SARSA and Q -learning, DQN can solve problems with large or continuous state space \n",
      "[24], mainly benefit ing from its two specific techniques.  \n",
      "Firstly, DQN uses the mechanism of experienc e replay to eliminate the correlation of \n",
      "network inputs. This means stor ing the transfer samples (𝑠,𝑎,𝑟,𝑠′) while the agent inter-\n",
      "acts with the environment and sampl es randomly to train the agent.  Secondly, there are \n",
      "two networks in DQN, where one is  the Q-network, and the other is the target network. \n",
      "These  two networks  have  the same  structure,  but have different parameters. The Q-net-\n",
      "work outputs the current Q value, and the target network outputs the target Q value. After \n",
      "some iterations, the parameters o f the Q-network are copied to the target network. The \n",
      "loss function is shown in Equation (4). \n",
      "𝐿(𝜃𝑖)=𝐸[(𝑟+ 𝛾max\n",
      "𝑎′𝑄(𝑠′,𝑎′ |𝜃𝑖−)−𝑄(𝑠,𝑎 |𝜃𝑖))2\n",
      "] (4) \n",
      "where 𝑎′ is the action selected in state 𝑠′, and 𝜃𝑖 and 𝜃𝑖− are the parameters of Q -net-\n",
      "work and target network, respectively.  \n",
      "4. Environment and Modeling  \n",
      "4.1. Cooling Water System Layout  \n",
      "In this paper, we tr ied to control the cooling water system to reduc e the energy con-\n",
      "sumption of the HVAC system. The cooling wat er system is an important part of HVAC, \n",
      "including chillers, cooling water pumps , cooling towers , and some other necessary equip-\n",
      "ment. To achieve the goal of energy saving, it is important to enable th is equipment to be \n",
      "controlled more efficiently. In anothe r word, we should try to find an optimal policy to \n",
      "coordinate th is equipment . Based on a  real application, we construct ed a cooling water \n",
      "system  platform, which contain ed four  chillers, three  cooling water pumps , and seven  \n",
      "cooling towers (the same type of equipment  has the  same settings), as shown in  Figure 3. \n",
      "Figure 2. Structure of deep forest.\n",
      "3.3. DQN\n",
      "Traditional methods in RL, such as SARSA and Q-learning, can effectively solve\n",
      "problems with a small state and action space by establishing Q-table. However, when\n",
      "the state space is large enough or continuous, such as practical problems in HVAC, these\n",
      "methods may fail to achieve a control policy. DQN, a method proposed by Google’s\n",
      "DeepMind in 2015 [ 23], has been applied in HVAC controls in recent years. Different from\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"methods may fail to achieve a control policy. DQN, a method proposed by Google’s\n",
      "DeepMind in 2015 [ 23], has been applied in HVAC controls in recent years. Different from\n",
      "SARSA and Q-learning, DQN can solve problems with large or continuous state space [ 24],\n",
      "mainly beneﬁting from its two speciﬁc techniques.\n",
      "Firstly, DQN uses the mechanism of experience replay to eliminate the correlation of\n",
      "network inputs. This means storing the transfer samples (s,a,r,s′)while the agent interacts\n",
      "with the environment and samples randomly to train the agent. Secondly, there are two\n",
      "networks in DQN, where one is the Q-network, and the other is the target network. These\n",
      "two networks have the same structure, but have different parameters. The Q-network\n",
      "outputs the current Q value, and the target network outputs the target Q value. After some\n",
      "iterations, the parameters of the Q-network are copied to the target network. The loss\n",
      "function is shown in Equation (4).\n",
      "L(θi)=E[(\n",
      "r+γmax\n",
      "a′Q(\n",
      "s′,a′|θ−\n",
      "i)−Q(s,a|θi))2]\n",
      "(4)\n",
      "where a′is the action selected in state s′, and θiandθ−\n",
      "iare the parameters of Q-network\n",
      "and target network, respectively.\n",
      "4. Environment and Modeling\n",
      "4.1. Cooling Water System Layout\n",
      "In this paper, we tried to control the cooling water system to reduce the energy\n",
      "consumption of the HVAC system. The cooling water system is an important part of\n",
      "HVAC, including chillers, cooling water pumps, cooling towers, and some other necessary\n",
      "equipment. To achieve the goal of energy saving, it is important to enable this equipment\n",
      "to be controlled more efﬁciently. In another word, we should try to ﬁnd an optimal policy\n",
      "to coordinate this equipment. Based on a real application, we constructed a cooling water\n",
      "system platform, which contained four chillers, three cooling water pumps, and seven\n",
      "cooling towers (the same type of equipment has the same settings), as shown in Figure 3.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 6 of 22\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW  6 of 22  \n",
      " \n",
      "Figure 3. The layout of the cooling water system.  \n",
      "To measure the effect of the control policy, we adopt ed the system coefficient of per-\n",
      "formance  (COP), which is often used to measure the energy -saving performance of HVAC \n",
      "systems. The system COP is defined in Equation (5). \n",
      "𝐶𝑂𝑃 = 𝐶𝐿𝑠𝑦𝑠𝑡𝑒𝑚\n",
      "∑𝑃𝑐ℎ𝑖𝑙𝑙𝑒𝑟𝑠 +∑𝑃𝑡𝑜𝑤𝑒𝑟𝑠 +∑𝑃𝑝𝑢𝑚𝑝𝑠 (5) \n",
      "where ∑𝑃𝑐ℎ𝑖𝑙𝑙𝑒𝑟𝑠  is the total power of all ch illers (kW), ∑𝑃𝑡𝑜𝑤𝑒𝑟𝑠  is the total power of all \n",
      "cooling towers (kW) , and ∑𝑃𝑝𝑢𝑚𝑝𝑠  is the total power of all cooling water pumps (kW). \n",
      "𝐶𝐿𝑠𝑦𝑠𝑡𝑒𝑚  is the system cooling load, which is defined in Equation (6). \n",
      "𝐶𝐿𝑠𝑦𝑠𝑦𝑡𝑒𝑚 =𝐶𝑝 × 𝜌 ×𝐹𝑐ℎ𝑤 ×(𝑇𝑐ℎ𝑤𝑟−𝑇𝑐ℎ𝑤𝑠)÷3600 sh⁄ (6) \n",
      "where  𝐶𝑝 is the specific heat capacity of water (4.2 𝑘𝐽/(kg∙K)), 𝜌 is the water density \n",
      "(1000  kg/m3), 𝐹𝑐ℎ𝑤 is the chilled water flowrate ( m3/h), 𝑇𝑐ℎ𝑤𝑟 is the inlet chilled water \n",
      "temperature of chillers ( °C), and 𝑇𝑐ℎ𝑤𝑠 is the outlet chilled water temperature of chillers \n",
      "(°C). \n",
      "4.2. System Simulation Modeling  \n",
      "For the system simulation, some real data and parameters were collected, but some \n",
      "others c ould not be achieved directly, so we tr ied to use  the regression method to attain  \n",
      "them . \n",
      "We regress ed the chiller model with historical data, which c ould  be used to attain  the \n",
      "chiller’s COP , and further, we calculated  the chiller’s power , as shown in  Equation (7). \n",
      "𝑃𝑐ℎ𝑖𝑙𝑙𝑒𝑟 =𝐶𝐿 / 𝐶𝑂𝑃𝑐ℎ𝑖𝑙𝑙𝑒𝑟 (7) \n",
      "where 𝐶𝑂𝑃𝑐ℎ𝑖𝑙𝑙𝑒𝑟 is obtained by Equation (8). \n",
      "𝐶𝑂𝑃𝑐ℎ𝑖𝑙𝑙𝑒𝑟 =𝑐ℎ𝑖𝑙𝑙𝑒𝑟  𝑚𝑜𝑑𝑒𝑙 (𝐶𝐿,𝑇𝑐𝑤𝑟,𝑇𝑐ℎ𝑤𝑠,𝐹𝑐ℎ𝑤) (8) \n",
      "where  𝑇𝑐𝑤𝑟 is the inlet cooling  water temperature of chillers ( °C). Some other related pa-\n",
      "rameters are shown in Equations (9)–(11) [8].  \n",
      "𝑇𝑐𝑤𝑠=𝑇𝑐𝑤𝑟+(𝑃𝑐ℎ𝑖𝑙𝑙𝑒𝑟 +𝐶𝐿)÷𝐶𝑝×𝐹𝑐𝑤 × 𝜌\n",
      "3600  s/h (9) \n",
      "𝑇𝑐ℎ𝑤𝑠=max [𝑇𝑐ℎ𝑤𝑠𝑠𝑒𝑡,𝑇𝑐ℎ𝑤𝑟′−𝐶𝐶÷𝐶𝑝 ×𝐹𝑐ℎ𝑤 ×𝜌\n",
      "3600  s/h] (10) \n",
      "Figure 3. The layout of the cooling water system.\n",
      "To measure the effect of the control policy, we adopted the system coefﬁcient of\n",
      "performance (COP), which is often used to measure the energy-saving performance of\n",
      "HVAC systems. The system COP is deﬁned in Equation (5).\n",
      "COP =CLsystem\n",
      "∑Pchillers +∑Ptowers +∑Ppumps(5)\n",
      "where ∑Pchillers is the total power of all chillers (kW), ∑Ptowers is the total power of all\n",
      "cooling towers (kW), and ∑Ppumps is the total power of all cooling water pumps (kW).\n",
      "CLsystem is the system cooling load, which is deﬁned in Equation (6).\n",
      "CLsystem =Cp×ρ×Fchw×(Tchwr−Tchws)÷3600 s/h (6)\n",
      "where Cpis the speciﬁc heat capacity of water ( 4.2kJ/(kg·K)),ρis the water density\n",
      "(1000 kg/m3),Fchwis the chilled water ﬂowrate ( m3/h),Tchwr is the inlet chilled water tem-\n",
      "perature of chillers (◦C), and Tchws is the outlet chilled water temperature of chillers (◦C).\n",
      "4.2. System Simulation Modeling\n",
      "For the system simulation, some real data and parameters were collected, but some oth-\n",
      "ers could not be achieved directly, so we tried to use the regression method to attain them.\n",
      "We regressed the chiller model with historical data, which could be used to attain the\n",
      "chiller’s COP , and further, we calculated the chiller’s power, as shown in Equation (7).\n",
      "Pchiller =CL/COP chiller (7)\n",
      "where COP chiller is obtained by Equation (8).\n",
      "COP chiller =chiller model (CL,Tcwr,Tchws,Fchw) (8)\n",
      "where Tcwris the inlet cooling water temperature of chillers (◦C). Some other related\n",
      "parameters are shown in Equations (9)–(11) [8].\n",
      "Tcws=Tcwr+(Pchiller +CL)÷Cp×Fcw×ρ\n",
      "3600 s/h(9)\n",
      "Tchws=max[\n",
      "Tchws set,T′\n",
      "chwr−CC÷Cp×Fchw×ρ\n",
      "3600 s/h]\n",
      "(10)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 7 of 22\n",
      "Tchwr=Tchws+CL÷Cp×Fchw×ρ\n",
      "3600 s/h(11)\n",
      "where Tchws setis the Tchws set point of a chiller, T′\n",
      "chwsis the Tchws of last time step, Fcwis the\n",
      "cooling water ﬂowrate (m3/h), and CC is the chiller cooling capacity.\n",
      "The power of the cooling water pump model is calculated by Equations (12) and (13).\n",
      "K=fpump actual\n",
      "fpump rated(12)\n",
      "Ppump=a+b×K+c×K2+d×K3(13)\n",
      "where fpump actualand fpump ratedare the actual running frequency and rated running fre-\n",
      "quency of the real cooling water pump, and a,b,c,dare determined by the regression of\n",
      "historical data.\n",
      "The cooling tower model is deﬁned as Equations (14) and (15).\n",
      "Ptower=a+b×ftower actual+c×ftower actual2+d×ftower actual3(14)\n",
      "Tcwr=tower model(\n",
      "Tcws,ftower actual,Twb,Fcw)\n",
      "(15)\n",
      "In Equation (14), ftower actualis the actual running frequency of cooling tower, a,b,c,d\n",
      "are determined by the regression of historical data. In Equation (15), Tcwris the inlet\n",
      "cooling water temperature of chillers (◦C),Tcwsis the outlet cooling water temperature\n",
      "of chillers (◦C),Twbis ambient wet-bulb temperature (◦C), and Fcwis the cooling water\n",
      "ﬂowrate (m3/h).\n",
      "For each model, we randomly selected 80% of the collected data set for training and\n",
      "20% of the data for testing, using MAPE (mean absolute percentage error) and CVRMSE\n",
      "(the coefﬁcient of variation of the root mean square error) as the error metrics to evaluate\n",
      "the accuracy of the models. All models had a MAPE of less than 5% and CVRMSE of less\n",
      "than 10%, which indicates that the accuracy of each model was within the acceptable range.\n",
      "The controller controlled the on and off states of this equipment and the operating\n",
      "frequency. An iterative process was as follows: Firstly, Tcwswas obtained from the CLand\n",
      "the switching state of the chiller model; then, Fcwwas obtained by combining the operating\n",
      "frequency and Tcws; ﬁnally, Tcwrwas obtained by combining the cooling tower model with\n",
      "Fcwand the Twet. All the parameters were iterated until Tcwrconverged (i.e., the difference\n",
      "ofTcwrbetween two successive iterations was less than 0.1◦C). If the Tcwrdid not converge\n",
      "within 50 iterations, the last result of Tcwrwas adopted and the iteration was stopped [ 8].\n",
      "The speciﬁc process is shown in Figure 4.\n",
      "4.3. Data Collection\n",
      "We used the data collected from the actual system to verify our proposed method. The\n",
      "details of this actual system corresponded with our simulation environment.\n",
      "We collected real data from 1 July 2021 to 10 October 2021, 102 days in total, where\n",
      "the sample interval was half an hour. The CLis shown in Figure 5, and the wet-bulb\n",
      "temperature is shown in Figure 6.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 8 of 22\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW  8 of 22  \n",
      " \n",
      "Figure 4. Simulation process.  \n",
      "4.3. Data Collection  \n",
      "We use d the data collected from the actual system to verify our proposed method. \n",
      "The details of this actual system corresponded with our simulation environment.  \n",
      "We collected real data from 1 July 2021 to 10 October 2021, 102 days in total, where \n",
      "the sample interval was half an hour. The 𝐶𝐿 is shown in Figure 5, and the wet -bulb tem-\n",
      "perature is shown in Figure 6. \n",
      " \n",
      "Figure 5. The temporal distribution of the cooling load. The deeper the color, the heavier the load.  \n",
      "Figure 4. Simulation process.\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW 8 of 22 \n",
      " \n",
      " \n",
      "Figure 4. Simulation process. \n",
      "4.3. Data Collection \n",
      "We used the data collected from the actual  system to verify our proposed method. \n",
      "The details of this actual  system corresponded with our simulation environment. \n",
      "We collected real data from 1 July 2021 to 10 October 2021, 102 days in total, where \n",
      "the sample interval was half an hour. The 𝐶𝐿 is shown in Figure 5, and the wet-bulb tem-\n",
      "perature is shown in Figure 6. \n",
      " \n",
      "Figure 5. The temporal distribution of the cooling load . The deeper the color, the heavier the load.  \n",
      "Figure 5. The temporal distribution of the cooling load. The deeper the color, the heavier the load.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 9 of 22\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW 9 of 22 \n",
      " \n",
      " \n",
      "Figure 6. The temporal distribution of the wet-bulb temperature. The deeper the color, the higher \n",
      "the temperature.  \n",
      "As shown in Figures 5 and 6, the darker the color, the greater the value of 𝐶𝐿 and \n",
      "the wet-bulb temperature ( 𝑇௪௘௧). From Figure 5, we can find that the main cooling demand \n",
      "of the system was concentrated between 6:00 and 23:00 every day. \n",
      "5. Methodology \n",
      "5.1. MDP Modeling \n",
      "Using RL methods for control problem requires MDP modeling of the environment. \n",
      "The details of the modeling are represented as follows: \n",
      "(a) State \n",
      "In this paper, we took the combination of ambient the wet-bulb temperature ( 𝑇௪௘௧) \n",
      "and system cooling load ( 𝐶𝐿 ௦௬௦௧௘௠ ) as state. There were two reasons for using these two \n",
      "variables: \n",
      "(1) The operation of the system has no  influence of these variables; \n",
      "(2) 𝐶𝐿 ௦௬௦௧௘௠  is a component factor of COP, which is related to the operation of cool-\n",
      "ing water system. \n",
      "(b) Action \n",
      "In this paper, operating frequencies of cooling tower fans and cooling water pumps \n",
      "were taken as the action (e.g.,  [𝑝𝑢𝑚𝑝_𝑎𝑐𝑡𝑖𝑜𝑛: 35 ℎ𝑧, 𝑡𝑜𝑤𝑒𝑟_𝑎𝑐𝑡𝑖𝑜𝑛: 35 ℎ𝑧] ). In addition, the \n",
      "action was discretized and the control accura cy was 1 hz. In order to protect the equip-\n",
      "ment, the action needed to be limited within  a reasonable range. We limited the action \n",
      "frequency within [20, 50] for both the cooling tower and cooling water pump, so there \n",
      "were 31 actions in total for each one. \n",
      "(c) Reward \n",
      "COP was taken as the reward in this paper. In the case of the same 𝐶𝐿 ௦௬௦௬௧௘௠ , the \n",
      "higher the COP value is, the sum of power is the lowest, which reflects the purpose of \n",
      "energy saving. The reward is shown in Equation (16). \n",
      "𝑅𝑒𝑤𝑎𝑟𝑑 =  𝐶𝑂𝑃 =  𝐶𝐿 ௦௬௦௧௘௠\n",
      "∑𝑃௖௛௜௟௟௘௥௦ +∑𝑃௧௢௪௘௥௦ +∑𝑃௣௨௠௣௦ (16) \n",
      "5.2. DF-DQN for Control \n",
      "Figure 7 depicts the overall framework of DF-DQN for control in cooling water sys-\n",
      "tem. Firstly, we labeled the collected stat e data, including cooling load and wet-bulb \n",
      "Figure 6. The temporal distribution of the wet-bulb temperature. The deeper the color, the higher\n",
      "the temperature.\n",
      "As shown in Figures 5 and 6, the darker the color, the greater the value of CLand the\n",
      "wet-bulb temperature ( Twet). From Figure 5, we can ﬁnd that the main cooling demand of\n",
      "the system was concentrated between 6:00 and 23:00 every day.\n",
      "5. Methodology\n",
      "5.1. MDP Modeling\n",
      "Using RL methods for control problem requires MDP modeling of the environment.\n",
      "The details of the modeling are represented as follows:\n",
      "(a) State\n",
      "In this paper, we took the combination of ambient the wet-bulb temperature ( Twet)\n",
      "and system cooling load ( CLsystem ) as state. There were two reasons for using these\n",
      "two variables:\n",
      "(1) The operation of the system has no inﬂuence of these variables;\n",
      "(2) CLsystem is a component factor of COP , which is related to the operation of\n",
      "cooling water system.\n",
      "(b) Action\n",
      "In this paper, operating frequencies of cooling tower fans and cooling water pumps\n",
      "were taken as the action (e.g., [pump _action : 35 hz, tower _action : 35 hz ]). In addi-\n",
      "tion, the action was discretized and the control accuracy was 1 hz. In order to protect\n",
      "the equipment, the action needed to be limited within a reasonable range. We limited\n",
      "the action frequency within [20, 50] for both the cooling tower and cooling water\n",
      "pump, so there were 31 actions in total for each one.\n",
      "(c) Reward\n",
      "COP was taken as the reward in this paper. In the case of the same CLsysytem , the\n",
      "higher the COP value is, the sum of power is the lowest, which reﬂects the purpose of\n",
      "energy saving. The reward is shown in Equation (16).\n",
      "Reward =COP =CLsystem\n",
      "∑Pchillers +∑Ptowers +∑Ppumps(16)\n",
      "5.2. DF-DQN for Control\n",
      "Figure 7 depicts the overall framework of DF-DQN for control in cooling water system.\n",
      "Firstly, we labeled the collected state data, including cooling load and wet-bulb temperature,\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 10 of 22\n",
      "according to the a priori knowledge. The label was the running frequency of the equipment\n",
      "more or less than the value of the base number under this state. If the operating frequency\n",
      "of the equipment under this state was less than base number, the label was ‘0’; otherwise,\n",
      "the label was ‘1’. The labeled state data were used to train the deep forest classiﬁcation\n",
      "model, of which 80% was used for training and 20% was used to test the accuracy of the\n",
      "trained model.\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW 10 of 22 \n",
      " \n",
      "temperature, according to the a priori knowledge. The label was the running frequency of \n",
      "the equipment more or less than the value of the base number under this state. If the op-\n",
      "erating frequency of the equipment under this  state was less than base number, the label \n",
      "was ‘0’; otherwise, the label was ‘1’. The la beled state data were used to train the deep \n",
      "forest classification model, of which 80% was used for training and 20% was used to test \n",
      "the accuracy of the trained model. \n",
      " \n",
      "Figure 7. Overall framework of DF-DQN for cooling water system.  \n",
      "After training, the DF classification model can output a label for the new state, which \n",
      "represents the relationship between the actu al frequency of equipment operation and the \n",
      "base number, and this label can be converted into a sign to shrink the action space there-\n",
      "after. Figure 8 gives more details. \n",
      " \n",
      "Figure 8. The process of  training a DF classifier.  \n",
      "Secondly, we obtained the relationship be tween the actual frequency of the equip-\n",
      "ment operation and base number under some states, namely the sign. We also needed the \n",
      "Figure 7. Overall framework of DF-DQN for cooling water system.\n",
      "After training, the DF classiﬁcation model can output a label for the new state, which\n",
      "represents the relationship between the actual frequency of equipment operation and the\n",
      "base number, and this label can be converted into a sign to shrink the action space thereafter.\n",
      "Figure 8 gives more details.\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW 10 of 22 \n",
      " \n",
      "temperature, according to the a priori knowledge. The label was the running frequency of \n",
      "the equipment more or less than the value of the base number under this state. If the op-\n",
      "erating frequency of the equipment under this  state was less than base number, the label \n",
      "was ‘0’; otherwise, the label was ‘1’. The la beled state data were used to train the deep \n",
      "forest classification model, of which 80% was used for training and 20% was used to test \n",
      "the accuracy of the trained model. \n",
      " \n",
      "Figure 7. Overall framework of DF-DQN for cooling water system.  \n",
      "After training, the DF classification model can output a label for the new state, which \n",
      "represents the relationship between the actu al frequency of equipment operation and the \n",
      "base number, and this label can be converted into a sign to shrink the action space there-\n",
      "after. Figure 8 gives more details. \n",
      " \n",
      "Figure 8. The process of  training a DF classifier.  \n",
      "Secondly, we obtained the relationship be tween the actual frequency of the equip-\n",
      "ment operation and base number under some states, namely the sign. We also needed the \n",
      "Figure 8. The process of training a DF classiﬁer.\n",
      "Secondly, we obtained the relationship between the actual frequency of the equipment\n",
      "operation and base number under some states, namely the sign. We also needed the\n",
      "difference between the operating frequency of the actual equipment and the base number.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 11 of 22\n",
      "In this part, we trained DQN agent, which output an action a′\n",
      "t, the absolute value of the\n",
      "difference between the true action and base number, namely ∆.\n",
      "At last, according to the actual data we collected, the DF classiﬁer output a positive\n",
      "sign or negative sign (‘+’ & ‘ −’), and DQN output ∆. Based on sign, ∆, and base number,\n",
      "we could obtain the actual equipment running frequency, namely action at. The actual\n",
      "action is calculated according to Equations (17) and (18).\n",
      "Sign =DFclassi f ier (state) (17)\n",
      "at=base number +Sign(\n",
      "a′\n",
      "t)\n",
      "(18)\n",
      "where sign is output by DF, a′\n",
      "tis output by DQN in DF-DQN.\n",
      "5.3. Theoretical Analysis of Shrink Action\n",
      "The DF classiﬁer labeled each state. It could replace the original action space with\n",
      "a smaller action space combined with the label, so as to realize the reduction of the\n",
      "action space.\n",
      "Owing to the introduction of DF in DQN, the original action space of each equip-\n",
      "ment was reduced from 31 to 16, so the original combined action pace was reduced from\n",
      "31×31 to 16×16. Therefore, the action space of each equipment was reduced by nearly\n",
      "half, while the combined action of the two equipment reduced the action space by nearly\n",
      "3/4 with the increase in equipment types. The introduction of DF could make the combined\n",
      "action space decrease exponentially. Figure 9 presents more details.\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW 11 of 22 \n",
      " \n",
      "difference between the operating frequency of  the actual equipment and the base number. \n",
      "In this part, we trained DQN agent, which output an action 𝑎௧ᇱ, the absolute value of the \n",
      "difference between the true action and base number, namely ∆. \n",
      "At last, according to the actual data we collected, the DF classifier output a positive \n",
      "sign or negative sign (‘+’ & ‘ −’), and DQN output ∆. Based on sign, ∆, and base number, \n",
      "we could obtain the actual equipment running frequency, namely action 𝑎௧. The actual \n",
      "action is calculated according to Equations (17) and (18). \n",
      "𝑆𝑖𝑔𝑛 = 𝐷𝐹 ௖௟௔௦௦௜௙௜௘௥ (𝑠𝑡𝑎𝑡𝑒 ) (17) \n",
      "𝑎௧= 𝑏𝑎𝑠𝑒 ௡௨௠௕௘௥ +𝑆 𝑖 𝑔 𝑛 (𝑎௧ᇱ) (18) \n",
      "where 𝑠𝑖𝑔𝑛  is output by DF, 𝑎௧ᇱ is output by DQN in DF-DQN. \n",
      "5.3. Theoretical Analys is of Shrink Action \n",
      "The DF classifier labeled each state. It coul d replace the original action space with a \n",
      "smaller action space combined with the label, so as to realize the reduction of the action \n",
      "space. \n",
      "Owing to the introduction of DF in DQN, the original action space of each equipment \n",
      "was reduced from 31 to 16, so the original combined action pace was reduced from 31 × \n",
      "31 to 16 × 16. Therefore, the action space of  each equipment was reduced by nearly half, \n",
      "while the combined action of the two equipmen t reduced the action space by nearly 3/4 \n",
      "with the increase in equipment types. The in troduction of DF could make the combined \n",
      "action space decrease exponentially.  Figure 9 presents more details. \n",
      " \n",
      "Figure 9. DF reduce action space.  \n",
      "Theoretically, if DF can divide each action into M categories with the same number, \n",
      "and the final combined actions include N kinds,  the reduced action space of DF-DQN can \n",
      "follow Equation (19). \n",
      "(𝐴𝑐𝑡𝑖𝑜𝑛 𝑠𝑝𝑎𝑐𝑒 )஽ிି஽ொே\n",
      "𝑂𝑟𝑖𝑔𝑖𝑛𝑎𝑙 𝑎𝑐𝑡𝑖𝑜𝑛 𝑠𝑝𝑎𝑐𝑒 ൎ ൬1\n",
      "𝑀൰ே\n",
      " (19) \n",
      "Based on the above analysis, when dealing with the problem with large action space \n",
      "or multiple action combinations, DF can significantly shrink the scale of the original action \n",
      "space, which can reduce the complexity of the problem to a certain extent finally. \n",
      "In this paper, 𝑀 =  2 , 𝑁 =  2 , so the combined action of the two equipment was \n",
      "shrunk into about 1/4 of the original action space. \n",
      "Figure 9. DF reduce action space.\n",
      "Theoretically, if DF can divide each action into M categories with the same number,\n",
      "and the ﬁnal combined actions include N kinds, the reduced action space of DF-DQN can\n",
      "follow Equation (19).\n",
      "(Action space )DF−DQN\n",
      "Original action space≈(1\n",
      "M)N\n",
      "(19)\n",
      "Based on the above analysis, when dealing with the problem with large action space\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"follow Equation (19).\n",
      "(Action space )DF−DQN\n",
      "Original action space≈(1\n",
      "M)N\n",
      "(19)\n",
      "Based on the above analysis, when dealing with the problem with large action space\n",
      "or multiple action combinations, DF can signiﬁcantly shrink the scale of the original action\n",
      "space, which can reduce the complexity of the problem to a certain extent ﬁnally.\n",
      "In this paper, M=2,N=2, so the combined action of the two equipment was shrunk\n",
      "into about 1/4 of the original action space.\n",
      "5.4. DF-DQN Algorithm\n",
      "The details of DF-DQN for the cooling water system is shown in Algorithm 1.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 12 of 22\n",
      "Algorithm 1. DF-DQN for cooling water system\n",
      "Initialize replay memory Dto capacity N\n",
      "Initialize action value function Qwith random weights θ\n",
      "Detect and replace outliers in training set\n",
      "Split the training set (80% for training, 20% for testing)\n",
      "Train the deep forest classiﬁer F\n",
      "For episode = 1, M do\n",
      "Attain initial state stof the cooling water system\n",
      "For t = 1, T do\n",
      "Select a random action a′\n",
      "twith probability ε, otherwise a′\n",
      "t=maxQ(st,a;θ)\n",
      "Attain positive or negative sign through F\n",
      "Combine base number, sign (‘+’ or ‘ −’),a′\n",
      "tto derive at(the true running frequency of cooling\n",
      "water system)\n",
      "Execute action atin cooling water system\n",
      "Observe reward rtand state st+1from the simulation system\n",
      "Store transition ( st,a′\n",
      "t,rt,st+1) inD\n",
      "Sample random minibatch of transitions ( sj,a′\n",
      "j,rj,sj+1) from D\n",
      "Setyj={rj f or terminal state s t+1\n",
      "rj+γmax\n",
      "a′Q(\n",
      "sj+1,a′;θ)\n",
      "otherwise\n",
      "Update Qfunction using(\n",
      "yi−Q(\n",
      "sj,aj;θ))2\n",
      "Copy parameters every Jsteps\n",
      "Update state st←st+1\n",
      "End for\n",
      "End for\n",
      "6. Experiment and Result\n",
      "To verify the performance of DF-DQN, we compared it with three other benchmark\n",
      "methods. In addition, we presented some experiments about the effect of DF accuracy on\n",
      "the performance of DF-DQN.\n",
      "6.1. Compare Methods\n",
      "1. DF-DQN: DF-DQN is the method we proposed before [ 25], which has been used to\n",
      "solve prediction problem. We extended DF-DQN to control problems in this paper;\n",
      "2. DQN: In this paper, DQN and DF-DQN share the same parameter settings in the\n",
      "DQN part. For the cooling water system, the action space was small and discrete, and\n",
      "its state space was large enough, so usually DQN can provide a good control policy\n",
      "according to paper [24];\n",
      "3. Baseline control: The PID control was selected as the baseline method, which is\n",
      "often used in real HVAC control applications. This method selects the action by\n",
      "approaching the difference between Tcwsand Tcwr. We took the baseline control\n",
      "method for comparison because it is the original control method in this system;\n",
      "4. Model-based control: The model-based control is the best method among all methods,\n",
      "and can select the best action in each situation, but this method is heavily dependent\n",
      "on the model. In this paper, we traversed the best action in each state as the model-\n",
      "based control. Actually, it is often impossible to deploy the model-based control\n",
      "method in real applications, but in this paper, based on our simulation model, the\n",
      "model-based control method provided the best policy. We used the model-based\n",
      "control method for comparison because it has the best control performance of all\n",
      "methods in this system.\n",
      "6.2. Parameters Setting\n",
      "We used DF-DQN and three other methods to control this system for comparison,\n",
      "including DQN, a baseline control, and a model-based control.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 13 of 22\n",
      "The agent in DF-DQN took the ε−greedy policy to select the action. At the beginning,\n",
      "we ensured the agent could explore the environment as much as possible, so we set\n",
      "εinital=1. We used a liner decay during the process, and set ∆ε=0.0001 ,εmin=0.01.\n",
      "In order to make the agent take more focus on the current COP , we set γ=0.01. The\n",
      "agent’s policy network and target network were both composed of two hidden layers. The\n",
      "minibatch was set to 32. The capacity of memory pooling ( Memory capacity ) was set to 1000,\n",
      "and we set Cstep(Copy steps) to 100, Cstep=100. The learning rate was set to 0.01, α=0.01.\n",
      "All parameters are shown in Table 1.\n",
      "Table 1. Parameters of DF-DQN and DQN.\n",
      "Parameters Value\n",
      "εinital 1\n",
      "∆ε 0.0001\n",
      "εmin 0.01\n",
      "γ 0.01\n",
      "Memory capacity 1000\n",
      "Cstep 100\n",
      "α 0.01\n",
      "In this system, the equipment contained chillers, cooling water pumps, and cooling\n",
      "towers. We used RL to control the cooling water pumps and cooling towers. As for the\n",
      "chillers, we used a sequence control [ 26] to reduce unnecessary refrigerating capacity, which\n",
      "can protect chillers at the same time. The workﬂow of this system is shown in Appendix A.\n",
      "6.3. Experimental Result\n",
      "In this paper, we used the model-based control to attain the best action in each state,\n",
      "so that we could attain the label of the cooling water pump and cooling tower’s action\n",
      "under each state. We used DF for two classiﬁcations to judge whether the frequency of\n",
      "the cooling water pump and cooling tower was more or less than the base number in each\n",
      "state. If it was more than the base number, we labeled it as 1 (represent ‘+’); otherwise, we\n",
      "labeled it as 0 (represent ‘ −’). The accuracy of DF can reach 97.319% and 99.694%. DF-DQN\n",
      "combined DF and DQN, where DF output sign ‘+’ or ‘ −’, and DQN of DF-DQN output\n",
      "∆, and then we combined them with base number to attain the ﬁnal action of the cooling\n",
      "water pump and cooling tower.\n",
      "Cumulative reward in an episode was taken to prove the convergence of DF-DQN.\n",
      "With the increase in episode, when the value of cumulative reward ﬂuctuated less, we\n",
      "believe that the method converged. One of the comparison methods, DQN, also used the\n",
      "same method. The reward was deﬁned by Equation (16), namely COP , and the higher\n",
      "reward not only conveyed that it had better converge, but also represented that the method\n",
      "had better energy-saving performance.\n",
      "We explain the experimental results from two aspects: one is the inﬂuence of DF’s\n",
      "accuracy on DF-DQN, and the other is control performance of DF-DQN.\n",
      "6.3.1. Inﬂuence of DF’s Accuracy on DF-DQN\n",
      "The accuracy of DF affected the performance of DF-DQN. In order to better explain\n",
      "the inﬂuence of DF accuracy on the performance of DF-DQN, we made a test with a low\n",
      "accuracy case. We used DF-DQN (false label) and DF-DQN to control the system for\n",
      "20 years in our simulation environment. We randomly generated labels to replace the\n",
      "original labels that DF generated, so that we could analyze the impact of DF accuracy on\n",
      "the performance of DF-DQN. The accuracy of the randomly generated labels was 50% of\n",
      "the original labels. We compared the experimental results of DF-DQN (false label) with the\n",
      "DF-DQN from three aspects: COP , cumulative power, and energy-saving effect.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 14 of 22\n",
      "Before comparison, we needed to ensure that both methods could converge. The\n",
      "convergence of the two methods is shown in Figure 10, where one episode in the training\n",
      "process is one year.\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW 14 of 22 \n",
      " \n",
      "6.3.1. Influence of DF’s Accuracy on DF-DQN \n",
      "The accuracy of DF affected the performa nce of DF-DQN. In order to better explain \n",
      "the influence of DF accuracy on the performa nce of DF-DQN, we made a test with a low \n",
      "accuracy case. We used DF-DQN (false label)  and DF-DQN to control the system for 20 \n",
      "years in our simulation environment. We rand omly generated labels to replace the origi-\n",
      "nal labels that DF generated, so that we co uld analyze the impact of DF accuracy on the \n",
      "performance of DF-DQN. The accuracy of the randomly generated la bels was 50% of the \n",
      "original labels. We compared the experimental  results of DF-DQN (false label) with the \n",
      "DF-DQN from three aspects: COP, cumulative power, and energy-saving effect. \n",
      "Before comparison, we needed to ensure that both methods could converge. The con-\n",
      "vergence of the two methods is shown in Figure 10, where one episode in the training process is one year. \n",
      " \n",
      "Figure 10. Comparison of cumulative reward between DF-DQN (false label) and DF-DQN.  \n",
      "DF-DQN (false label) and DF-DQN both converged at last. Although the wrong label \n",
      "was used, DQN still learned the control policy under the wrong labels and converged. However, the cumulative reward of DF-DQN was higher than that of DF-DQN (false la-\n",
      "bel) on the whole, which also reflected th e better performance of DF-DQN. In addition, \n",
      "the performance of DF-DQN (false label) decreased a lot due to the false labels. \n",
      "As shown in Figure 11, the COP of DF-DQN (false label) was lower than that of DF-\n",
      "DQN in 20 years, which indicates that the control performance of DF-DQN (false label) was worse than that of DF-DQN in each year , and the energy-saving effect decreased ac-\n",
      "cordingly, which also can be found in the cumulative power compar ison in Figure 12. \n",
      "Figure 10. Comparison of cumulative reward between DF-DQN (false label) and DF-DQN.\n",
      "DF-DQN (false label) and DF-DQN both converged at last. Although the wrong label\n",
      "was used, DQN still learned the control policy under the wrong labels and converged.\n",
      "However, the cumulative reward of DF-DQN was higher than that of DF-DQN (false label)\n",
      "on the whole, which also reﬂected the better performance of DF-DQN. In addition, the\n",
      "performance of DF-DQN (false label) decreased a lot due to the false labels.\n",
      "As shown in Figure 11, the COP of DF-DQN (false label) was lower than that of\n",
      "DF-DQN in 20 years, which indicates that the control performance of DF-DQN (false label)\n",
      "was worse than that of DF-DQN in each year, and the energy-saving effect decreased\n",
      "accordingly, which also can be found in the cumulative power comparison in Figure 12.\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW 16 of 24 \n",
      " \n",
      "  \n",
      "Figure 11. Comparison of COP between DQN and DF-DQN.  \n",
      " \n",
      " \n",
      "Figure 12. Comparison of cumulative power between  DF-DQN and DF-DQN (false label).  \n",
      "We compared the energy-saving effect of these two methods, and used the baseline \n",
      "control method of the system as a benchmark. The partial energy-saving effect compari-\n",
      "son can be found in Table 2. \n",
      "Table 2. Partial energy-saving effect comparison result. \n",
      "Energy Saving (Compared to Baseline Control) \n",
      "Year DF-DQN DF-DQN (False Label) \n",
      "1st 8.074% −10.022% \n",
      "2nd 11.337% −8.037% \n",
      "3rd  10.086% −4.224% \n",
      "5th 11.157% −5.191% \n",
      "10th  11.580% −1.934% \n",
      "15th  12.168% −3.754% \n",
      "20th  10.177% −4.094% \n",
      "Figure 11. Comparison of COP between DQN and DF-DQN.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 15 of 22\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW 15 of 22 \n",
      " \n",
      " \n",
      "Figure 11. Comparison of COP between DQN and DF-DQN.  \n",
      " \n",
      "Figure 12. Comparison of cumulative power betw een DF-DQN and DF-DQN (false label).  \n",
      "We compared the energy-saving effect of these two methods, and used the baseline \n",
      "control method of the system as a benchmar k. The partial energy-saving effect compari-\n",
      "son can be found in Table 2. \n",
      "Table 2. Partial energy-saving effect comparison result. \n",
      "Energy Saving (Compared to Baseline Control) \n",
      "Year DF-DQN DF-DQN (False Label) \n",
      "1st 8.074% −10.022% \n",
      "2nd 11.337% −8.037% \n",
      "3rd  10.086% −4.224% \n",
      "5th 11.157% −5.191% \n",
      "10th  11.580% −1.934% \n",
      "15th  12.168% −3.754% \n",
      "20th  10.177% −4.094% \n",
      "Average (20 years)  11.035%  −4.104%  \n",
      "Regardless of the comparison of COP, th e cumulative power, or the energy-saving \n",
      "effect, DF-DQN (false label) was worse than DF-DQN. The direct reason for this result \n",
      "Figure 12. Comparison of cumulative power between DF-DQN and DF-DQN (false label).\n",
      "We compared the energy-saving effect of these two methods, and used the baseline\n",
      "control method of the system as a benchmark. The partial energy-saving effect comparison\n",
      "can be found in Table 2.\n",
      "Table 2. Partial energy-saving effect comparison result.\n",
      "Energy Saving (Compared to Baseline Control)\n",
      "Year DF-DQN DF-DQN (False Label)\n",
      "1st 8.074% −10.022%\n",
      "2nd 11.337% −8.037%\n",
      "3rd 10.086% −4.224%\n",
      "5th 11.157% −5.191%\n",
      "10th 11.580% −1.934%\n",
      "15th 12.168% −3.754%\n",
      "20th 10.177% −4.094%\n",
      "Average (20 years) 11.035% −4.104%\n",
      "Regardless of the comparison of COP , the cumulative power, or the energy-saving\n",
      "effect, DF-DQN (false label) was worse than DF-DQN. The direct reason for this result was\n",
      "the wrong labels. From the comparison result, we found that the accuracy of DF directly\n",
      "affected the performance of DF-DQN, and the low accuracy of DF led to a decrease in the\n",
      "performance of DF-DQN. Therefore, for DF-DQN control in this problem, it was crucial to\n",
      "improve the accuracy of DF as much as possible.\n",
      "6.3.2. Performance of DF-DQN Compared with DQN, Baseline Control, and\n",
      "Model-Based Control\n",
      "DF-DQN and DQN both converged at last, but in the beginning episodes, DF-DQN\n",
      "achieved a higher cumulative reward. The difference between DF-DQN and DQN can be\n",
      "found in Figure 13 more clearly.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 16 of 22\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW 16 of 22 \n",
      " \n",
      "was the wrong labels. From the comparison result, we found that the accuracy of DF di-\n",
      "rectly affected the performance of DF-DQN, an d the low accuracy of DF led to a decrease \n",
      "in the performance of DF-DQN. Therefore, for DF-DQN control in this problem, it was crucial to improve the accuracy of DF as much as possible. \n",
      "6.3.2. Performance of DF-DQN Compared with DQN, Baseline Control, and Model-\n",
      "Based Control \n",
      "DF-DQN and DQN both converged at last, but in the beginning episodes, DF-DQN \n",
      "achieved a higher cumulative reward. The difference between DF-DQN and DQN can be \n",
      "found in Figure 13 more clearly. \n",
      " \n",
      "Figure 13. Comparison of cumulative reward between DQN and DF-DQN.  \n",
      "As shown in Figure 13, the cumulative re ward of DF-DQN was much greater than \n",
      "that of DQN before the fifth episode, which reflects that the control performance of DF-\n",
      "DQN was much better than DQN in the early st age. After the fifth episode, DQN outper-\n",
      "formed DF-DQN, which was due to the accuracy of DF. However, the performance of DF-\n",
      "DQN was almost approaching DQN. \n",
      "In order to compare the performance of the baseline control method, DQN, DF-DQN, \n",
      "and the model-based control method, we used them to control the system for 20 years in \n",
      "our simulation environment. We compared thei r performance in three aspects: the COP, \n",
      "the cumulative power, and the energy-saving effect. \n",
      "(a) COP \n",
      "The COP is shown in Figure 14. The model-based control method was the best \n",
      "method among these methods in theory, and its COP was the highest in practice. The \n",
      "baseline control method is a relatively poor control method compared with others, and its COP was the lowest in most of the years. \n",
      "Figure 13. Comparison of cumulative reward between DQN and DF-DQN.\n",
      "As shown in Figure 13, the cumulative reward of DF-DQN was much greater than that\n",
      "of DQN before the ﬁfth episode, which reﬂects that the control performance of DF-DQN\n",
      "was much better than DQN in the early stage. After the ﬁfth episode, DQN outperformed\n",
      "DF-DQN, which was due to the accuracy of DF. However, the performance of DF-DQN\n",
      "was almost approaching DQN.\n",
      "In order to compare the performance of the baseline control method, DQN, DF-DQN,\n",
      "and the model-based control method, we used them to control the system for 20 years in\n",
      "our simulation environment. We compared their performance in three aspects: the COP ,\n",
      "the cumulative power, and the energy-saving effect.\n",
      "(a) COP\n",
      "The COP is shown in Figure 14. The model-based control method was the best method\n",
      "among these methods in theory, and its COP was the highest in practice. The baseline\n",
      "control method is a relatively poor control method compared with others, and its COP was\n",
      "the lowest in most of the years.\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW 18 of 24 \n",
      " \n",
      "  \n",
      "Figure 14. COP comparison of each method.  \n",
      "From Figure 14, we found that the COP obtained by DQN and DF-DQN was gradu-\n",
      "ally becoming higher, indicating that their energy-saving effect was gradually becoming \n",
      "better, which is just as we have mentioned before, and the higher system COP, the better \n",
      "the energy-saving performance. The distribution of COP in the first year reflects that the control effect of DQN was much worse than that of DF-DQN, but it gradually became better in the later years. The COP reflected its better energy-saving performance to a cer-tain extent, but not absolutely. In addition, the minimum COP obtained by DQN and DF-\n",
      "DQN was relatively small, which was due to the poorly selected actions in a few states. \n",
      "As for DQN, its COP was less than the baseli ne control method in the first year, and \n",
      "the distribution of COP in the first year was also relatively scattered, but in the second \n",
      "year, the distribution of COP became concentrated, and its COP was more than the base-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"the distribution of COP in the first year was also relatively scattered, but in the second \n",
      "year, the distribution of COP became concentrated, and its COP was more than the base-\n",
      "line control method, which meant that DQN’s control performance became better in the second year. Finally, DQN’s COP became stable, which means that the control policy of \n",
      "DQN was becoming stable and convergent. \n",
      "In contrast with DQN, DF-DQN’s COP was between the baseline control method and \n",
      "model-based control method from the first year and this trend remained in the following \n",
      "20 years, which reflected that DF-DQN can obtain a better control effect from the begin-ning, and the control effect was better than DQN in the early stage. Moreover, the perfor-\n",
      "mance of DF-DQN was more stable than DQN in 20 years. \n",
      "The performance of DQN was not good in the early stage, and its COP was not be-\n",
      "tween the baseline control method and model-based control method until the policy con-\n",
      "verged. In contrast, DF-DQN met this condition not only after the convergence of the con-trol policy, but also from the very beginning, which reflected that DF-DQN converged faster than DQN, and had a better performance than DQN in the early stage. \n",
      "(b) Cumulative power \n",
      "In order to intuitively analyze the energy-saving effect of these four methods, we \n",
      "compared the annual cumulative power under these four methods’ control policies, and the comparison results are shown in Figure 15. \n",
      " \n",
      "Figure 14. COP comparison of each method.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 17 of 22\n",
      "From Figure 14, we found that the COP obtained by DQN and DF-DQN was gradually\n",
      "becoming higher, indicating that their energy-saving effect was gradually becoming better,\n",
      "which is just as we have mentioned before, and the higher system COP , the better the\n",
      "energy-saving performance. The distribution of COP in the ﬁrst year reﬂects that the\n",
      "control effect of DQN was much worse than that of DF-DQN, but it gradually became\n",
      "better in the later years. The COP reﬂected its better energy-saving performance to a certain\n",
      "extent, but not absolutely. In addition, the minimum COP obtained by DQN and DF-DQN\n",
      "was relatively small, which was due to the poorly selected actions in a few states.\n",
      "As for DQN, its COP was less than the baseline control method in the ﬁrst year, and\n",
      "the distribution of COP in the ﬁrst year was also relatively scattered, but in the second\n",
      "year, the distribution of COP became concentrated, and its COP was more than the baseline\n",
      "control method, which meant that DQN’s control performance became better in the second\n",
      "year. Finally, DQN’s COP became stable, which means that the control policy of DQN was\n",
      "becoming stable and convergent.\n",
      "In contrast with DQN, DF-DQN’s COP was between the baseline control method and\n",
      "model-based control method from the ﬁrst year and this trend remained in the following 20\n",
      "years, which reﬂected that DF-DQN can obtain a better control effect from the beginning,\n",
      "and the control effect was better than DQN in the early stage. Moreover, the performance\n",
      "of DF-DQN was more stable than DQN in 20 years.\n",
      "The performance of DQN was not good in the early stage, and its COP was not\n",
      "between the baseline control method and model-based control method until the policy\n",
      "converged. In contrast, DF-DQN met this condition not only after the convergence of the\n",
      "control policy, but also from the very beginning, which reﬂected that DF-DQN converged\n",
      "faster than DQN, and had a better performance than DQN in the early stage.\n",
      "(b) Cumulative power\n",
      "In order to intuitively analyze the energy-saving effect of these four methods, we\n",
      "compared the annual cumulative power under these four methods’ control policies, and\n",
      "the comparison results are shown in Figure 15.\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW 18 of 22 \n",
      " \n",
      " \n",
      "Figure 15. Cumulative power of each method in 20 years.  \n",
      "In Figure 15, the model-based control me thod had the lowest cumulative power, \n",
      "which is consistent with the intuition. The baseline control method had a relatively poor \n",
      "energy-saving effect, and its cumulative power was relatively more than others. \n",
      "The magnitude of DQN’s cumulative power wa s larger than that of the model-based \n",
      "control method and less than the baseline co ntrol method from the third year. In Figure \n",
      "14, though DQN’s COP was more than the base line control method in the second year, its \n",
      "cumulative power was still more than the base line control method. In Figure 15, the cu-\n",
      "mulative power of DQN in the second year was less than that in the first year, and it con-\n",
      "tinued to decrease until the fourth year, an d then remained stable. As we mentioned in \n",
      "the previous part, the lower COP did not abso lutely mean a better energy-saving effect, \n",
      "but from the experimental results, we found that the second year’s policy was better than \n",
      "the first year’s, which is also be conveyed in Figure 15. \n",
      "DF-DQN’s cumulative power was much lower than the baseline control method from \n",
      "the first year, and this trend continued to de crease until the second year and then re-\n",
      "mained stable. In addition, DF-DQN’s cumula tive power was also much less than DQN’s \n",
      "in the first three years, and after that, it almost approached DQN. It is obvious that DF-\n",
      "DQN can not only achieve a good energy-savin g effect, but can also save energy from an \n",
      "early stage. Compared with DQN, the energy-sa ving effect of DF-DQN in the early stage \n",
      "was much better. \n",
      "(c) Energy saving\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"early stage. Compared with DQN, the energy-sa ving effect of DF-DQN in the early stage \n",
      "was much better. \n",
      "(c) Energy saving \n",
      "Taking the baseline control method as th e benchmark, we compared the other three \n",
      "methods’ energy-saving effects in each year. The partial comparison results are repre-sented in Table 3, and the complete comp arison results are shown in Appendix B. \n",
      "Table 3. Partial comparison effect results. \n",
      "Energy Saving (Compared to Baseline Control) \n",
      "Year DQN DF-DQN Model-Based Control \n",
      "1st −29.996% 8.074% 13.755% \n",
      "2nd −9.843% 11.337% 13.755% \n",
      "3rd 0.798% 10.086% 13.755% \n",
      "5th 12.195% 11.157% 13.755% \n",
      "10th 11.908% 11.580% 13.755% 15th 11.461% 12.168% 13.755% 20th 12.094% 10.177% 13.755% \n",
      "Average (20 years) 7.972% 11.035% 13.755% \n",
      "Figure 15. Cumulative power of each method in 20 years.\n",
      "In Figure 15, the model-based control method had the lowest cumulative power,\n",
      "which is consistent with the intuition. The baseline control method had a relatively poor\n",
      "energy-saving effect, and its cumulative power was relatively more than others.\n",
      "The magnitude of DQN’s cumulative power was larger than that of the model-based\n",
      "control method and less than the baseline control method from the third year. In Figure 14,\n",
      "though DQN’s COP was more than the baseline control method in the second year, its\n",
      "cumulative power was still more than the baseline control method. In Figure 15, the\n",
      "cumulative power of DQN in the second year was less than that in the ﬁrst year, and it\n",
      "continued to decrease until the fourth year, and then remained stable. As we mentioned in\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 18 of 22\n",
      "the previous part, the lower COP did not absolutely mean a better energy-saving effect, but\n",
      "from the experimental results, we found that the second year’s policy was better than the\n",
      "ﬁrst year’s, which is also be conveyed in Figure 15.\n",
      "DF-DQN’s cumulative power was much lower than the baseline control method from\n",
      "the ﬁrst year, and this trend continued to decrease until the second year and then remained\n",
      "stable. In addition, DF-DQN’s cumulative power was also much less than DQN’s in the\n",
      "ﬁrst three years, and after that, it almost approached DQN. It is obvious that DF-DQN\n",
      "can not only achieve a good energy-saving effect, but can also save energy from an early\n",
      "stage. Compared with DQN, the energy-saving effect of DF-DQN in the early stage was\n",
      "much better.\n",
      "(c) Energy saving\n",
      "Taking the baseline control method as the benchmark, we compared the other three\n",
      "methods’ energy-saving effects in each year. The partial comparison results are represented\n",
      "in Table 3, and the complete comparison results are shown in Appendix B.\n",
      "Table 3. Partial comparison effect results.\n",
      "Energy Saving (Compared to Baseline Control)\n",
      "Year DQN DF-DQNModel-Based\n",
      "Control\n",
      "1st −29.996% 8.074% 13.755%\n",
      "2nd −9.843% 11.337% 13.755%\n",
      "3rd 0.798% 10.086% 13.755%\n",
      "5th 12.195% 11.157% 13.755%\n",
      "10th 11.908% 11.580% 13.755%\n",
      "15th 11.461% 12.168% 13.755%\n",
      "20th 12.094% 10.177% 13.755%\n",
      "Average (20 years) 7.972% 11.035% 13.755%\n",
      "There is no doubt that the model-based control method had the best energy-saving\n",
      "effect, reaching 13.775%. The energy-saving effect of DQN and DF-DQN both had a growth\n",
      "process before the convergence.\n",
      "According to the experimental results, DQN could not achieve the goal of energy\n",
      "saving until the third year. In particular, in the ﬁrst year, its energy-saving effect was\n",
      "29.996% worse than the baseline control method. In the second year, DQN’s saving effect\n",
      "became much better than the ﬁrst year, but was still 9.843% worse than the baseline control.\n",
      "Until the third year, DQN’s saving effect was 0.798% better than the baseline control\n",
      "method, and began to remain stable from the fourth year, and was able to achieve a 10–12%\n",
      "energy-saving effect each year. DQN’s energy-saving effect was not good in the early\n",
      "stage, but it became better and better with training. After 20 years control, its average\n",
      "energy-saving effect reaches 7.972%.\n",
      "In contrast, DF-DQN could achieve the goal of energy saving from the ﬁrst year, and\n",
      "remained with a 10–11% energy-saving effect. In the ﬁrst year, it could achieve 8.074%\n",
      "better than the baseline control method, and kept becoming better in the second year,\n",
      "reaching 11.337%. After 20 years of the control, its average energy-saving effect reached\n",
      "about 11.035%.\n",
      "DQN may have a better energy-saving effect in the later years, but it has to explore\n",
      "the environment before converging, which led to its worse performance in the early stage.\n",
      "Considering of the service life of the equipment, DF-DQN may have a better energy-saving\n",
      "effect than DQN in general, and our experimental results also proved this.\n",
      "7. Conclusions and Future Work\n",
      "In this paper, we extended DF-DQN from the prediction problem to the control\n",
      "problem, which was used to achieve the goal of energy saving with respect to the cooling\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 19 of 22\n",
      "water system control in HVAC. We compare its performance with DQN, baseline control\n",
      "method and the model-based control method. The experimental results show that since\n",
      "the a priori knowledge was introduced as a deep forest classiﬁer, DF-DQN’s action space\n",
      "could be mapped to a smaller one. DF-DQN did not need to spend a lot of on exploring the\n",
      "environment, so it converged much faster than DQN, which is the main reason that DF-\n",
      "DQN shows a better performance in the early stage compared to DQN. In the latter stage,\n",
      "the performance of DF-DQN was always slightly worse than DQN, and the reason is that\n",
      "the DF classiﬁer may have output some wrong labels in a few states, which directly affected\n",
      "the result and DF-DQN’s performance. Compared with the model-based control method,\n",
      "DF-DQN performed slightly worse in saving energy, but it did not require any complete\n",
      "system model, thus avoiding the unnecessary cost of modeling, which was valuable in the\n",
      "engineering practice.\n",
      "DF-DQN had obvious energy-saving effects in the early stage and the overall energy-\n",
      "saving effect was also good, but its performance was directly affected by DF, which relied\n",
      "on historical data or expert experience. Thus, it is particularly important to train a DF clas-\n",
      "siﬁer with excellent performance. DF-DQN has a good energy-saving effect in engineering\n",
      "applications, and is more practical than traditional RL methods, but it is not suitable for\n",
      "systems lacking historical data or expert experience. In addition, in this paper, we only\n",
      "considered two controllable equipment, but if more equipment need to be controlled, for\n",
      "example, more than 10 equipment, the performance of DF-DQN might decrease, which is\n",
      "limited by the DQN. Thus, for the future works, we will focus on following two aspects:\n",
      "(1) improving the accuracy of DF classiﬁer or constructing a new classiﬁer with higher\n",
      "accuracy, which could improve the ﬁnal control performance in the current DF-DQN frame-\n",
      "work. (2) When more equipment of different types is involved, multi-agent reinforcement\n",
      "learning method can be adopted into the DF-DQN framework.\n",
      "Author Contributions: Conceptualization, Z.H.; data curation, Z.H., Q.F. and Y.L.; formal analy-\n",
      "sis, Z.H. and Q.F.; funding acquisition, Q.F., J.C. and Y.W.; investigation, J.C., Y.W., Y.L., H.W. and\n",
      "H.G.; methodology, Z.H. and Q.F.; project administration, J.C.; software, Z.H. and Q.F.; supervi-\n",
      "sion, Q.F., J.C., Y.W., Y.L., H.W. and H.G.; validation, Z.H. and Q.F.; writing—original draft, Z.H.;\n",
      "writing—review and editing, Z.H., Q.F., Y.W., Y.L., H.W. and H.G. All authors have read and agreed\n",
      "to the published version of the manuscript.\n",
      "Funding: This work was ﬁnancially supported by National Key R&D Program of China\n",
      "(No. 2020YFC2006602), National Natural Science Foundation of China (No. 62172324, No. 62072324,\n",
      "No. 61876217, No. 61876121), University Natural Science Foundation of Jiangsu Province\n",
      "(No. 21KJA520005), Primary Research and Development Plan of Jiangsu Province (No. BE2020026),\n",
      "Natural Science Foundation of Jiangsu Province (No. BK20190942).\n",
      "Institutional Review Board Statement: Not applicable.\n",
      "Informed Consent Statement: Not applicable.\n",
      "Data Availability Statement: The experiment results are available at: https://github.com/H-\n",
      "Phoebe/DF-DQN-for-energy-saving-control (accessed on 20 August 2022).\n",
      "Acknowledgments: The authors appreciate the support of Shunian Qiu.\n",
      "Conﬂicts of Interest: The authors declare no conﬂict of interest.\n",
      "Appendix A\n",
      "The workﬂow of this system:\n",
      "The workﬂow can be described as following steps:\n",
      "A. In time step t, the agent observes the state st, and decides to turn the system on or\n",
      "off according to CL. This process is shown in the right-hand part of Figure A1. The\n",
      "details of this process are shown below:\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 20 of 22\n",
      "Buildings 2022 , 12, x FOR PEER REVIEW 20 of 22 \n",
      " \n",
      "J.C., Y.W., Y.L., H.W. and H.G.; validation, Z.H. and Q.F.; writing—original draft, Z.H.; writing—\n",
      "review and editing, Z.H., Q.F., Y.W., Y.L., H.W. an d H.G. All authors have read and agreed to the \n",
      "published version of the manuscript. \n",
      "Funding:  This work was financially supported by National Key R&D Program of China (No. \n",
      "2020YFC2006602), National Natural Science Founda tion of China (No. 62172324, No. 62072324, No. \n",
      "61876217, No. 61876121), University Natural Science Foundation of Jiangsu Province (No. \n",
      "21KJA520005), Primary Research and Development Pl an of Jiangsu Province (No. BE2020026), Nat-\n",
      "ural Science Foundation of Jiangsu Province (No. BK20190942). \n",
      "Institutional Review Board Statement: Not applicable. \n",
      "Informed Consent Statement: Not applicable. \n",
      "Data Availability Statement: The experiment results are available at: https://github.com/H -\n",
      "Phoebe/DF-DQN-for-energy-saving-contr ol (accessed on 20 August 2022). \n",
      "Acknowledgments: The authors appreciate the support of Shunian Qiu.  \n",
      "Conflicts of Interest: The authors declare no conflict of interest.  \n",
      "Appendix A \n",
      "The workflow of this system: \n",
      "The workflow can be described as following steps: \n",
      "A. In time step 𝑡, the agent observes the state 𝑠௧, and decides to turn the system on or \n",
      "off according to 𝐶𝐿. This process is shown in the right-hand part of Figure A1. The \n",
      "details of this process are shown below: \n",
      " \n",
      "Figure A1. The workflow of system.  \n",
      "(1) If 𝐶𝐿 is less than 20% of the chiller cooling capacity (CC) that one chiller can \n",
      "offer, the system will be turned off; \n",
      "(2) If 𝐶𝐿 is more than 20% of the rated refrig erating capacity that one chiller can \n",
      "offer and less than the refrigerating capacity that all the chillers can offer, namely 4×CC, we will turn on the system, and the number of chillers is decided by the \n",
      "minimum 𝑥, which can make 𝑥 × 𝐶𝐶 ൒ 𝐶𝐿 , and 𝑥 is the number of chillers we \n",
      "turn on. 𝑥 can be calculated by Equation (A1). \n",
      "𝑥 = 𝐶𝐿 // 𝐶𝐶 + 1  (A1) \n",
      "Figure A1. The workﬂow of system.\n",
      "(1) If CLis less than 20% of the chiller cooling capacity (CC) that one chiller can\n",
      "offer, the system will be turned off;\n",
      "(2) IfCLis more than 20% of the rated refrigerating capacity that one chiller can\n",
      "offer and less than the refrigerating capacity that all the chillers can offer,\n",
      "namely 4×CC, we will turn on the system, and the number of chillers is\n",
      "decided by the minimum x, which can make x×CC≥CL, and xis the\n",
      "number of chillers we turn on. xcan be calculated by Equation (A1).\n",
      "x=CL//CC+1 (A1)\n",
      "where “ //” represents exact division. No matter how many chillers we turn\n",
      "on, the CLassigned to each chiller is the same. As for cooling water pumps\n",
      "and the cooling towers, we turn on 2 and 4, respectively.\n",
      "(3) IfCLis more than 4×CC, we turn on all the chillers, cooling water pumps,\n",
      "and cooling towers, namely 4, 3, 7, respectively.\n",
      "B. We use the DF-DQN controller to control cooling water pumps and cooling towers, select\n",
      "the frequency of them, and combine them into an action (pump _action ,tower _action ).\n",
      "The system COP , reward in RL, can be observed after executing the action. The action\n",
      "is selected by ε−greedy policy;\n",
      "C. Then we train our DF-DQN agent;\n",
      "D. Transfer to next state st+1;\n",
      "E. End the current learning and move to step (A).\n",
      "Appendix B\n",
      "The energy-saving effects obtained by all methods in this paper are shown in Table A1.\n",
      "Table A1. Energy-saving effect of each method compared with baseline control.\n",
      "Energy Saving (Compared to Baseline Control)\n",
      "Year DQN DF-DQN DF-DQN (False Label) Model-Based Control\n",
      "1st−29.996% 8.074% −10.022% 13.755%\n",
      "2nd−9.843% 11.337% −8.037% 13.755%\n",
      "3rd 0.798% 10.086% −4.224% 13.755%\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 21 of 22\n",
      "Table A1. Cont.\n",
      "Energy Saving (Compared to Baseline Control)\n",
      "Year DQN DF-DQN DF-DQN (False Label) Model-Based Control\n",
      "4th 11.362% 11.273% −5.659% 13.755%\n",
      "5th 12.195% 11.157% −5.191% 13.755%\n",
      "6th 11.752% 11.677% −2.374% 13.755%\n",
      "7th 11.879% 11.480% −3.465% 13.755%\n",
      "8th 12.503% 11.578% −2.349% 13.755%\n",
      "9th 8.957% 11.757% −3.350% 13.755%\n",
      "10th 11.908% 11.580% −1.934% 13.755%\n",
      "11th 12.440% 11.636% −2.274% 13.755%\n",
      "12th 11.195% 10.299% −2.866% 13.755%\n",
      "13th 11.763% 10.879% −2.266% 13.755%\n",
      "14th 10.893% 10.311% −3.364% 13.755%\n",
      "15th 11.461% 12.168% −3.754% 13.755%\n",
      "16th 12.042% 11.855% −2.420% 13.755%\n",
      "17th 10.967% 11.850% −7.476% 13.755%\n",
      "18th 12.583% 10.639% −3.268% 13.755%\n",
      "19th 12.490% 10.893% −3.704% 13.755%\n",
      "20th 12.094% 10.177% −4.094% 13.755%\n",
      "Average 7.972% 11.035% −4.104% 13.755%\n",
      "References\n",
      "1. Cao, X.; Dai, X.; Liu, J. Building energy-consumption status worldwide and the state-of-the-art technologies for zero-energy\n",
      "buildings during the past decade. Energy Build. 2016 ,128, 198–213. [CrossRef]\n",
      "2. Taylor, S.T. Fundamentals of Design and Control of Central Chilled-Water Plants ; ASHRAE Learning Institute: Atlanta, GA, USA, 2017.\n",
      "3. Wang, S.; Ma, Z. Supervisory and optimal control of building HVAC systems: A review. HV AC&R Res. 2008 ,14, 3–32. [CrossRef]\n",
      "4. Wang, J.; Hou, J.; Chen, J.; Fu, Q.; Huang, G. Data mining approach for improving the optimal control of HVAC systems: An\n",
      "event-driven strategy. J. Build. Eng. 2021 ,39, 102246. [CrossRef]\n",
      "5. Gholamzadehmir, M.; Del Pero, C.; Buffa, S.; Fedrizzi, R.; Aste, N. Adaptive-predictive control strategy for HVAC systems in\n",
      "smart buildings–A review. Sustain. Cities Soc. 2020 ,63, 102480. [CrossRef]\n",
      "6. Zhu, N.; Shan, K.; Wang, S.; Sun, Y. An optimal control strategy with enhanced robustness for air-conditioning systems considering\n",
      "model and measurement uncertainties. Energy Build. 2013 ,67, 540–550. [CrossRef]\n",
      "7. Heo, Y.; Choudhary, R.; Augenbroe, G.A. Calibration of building energy models for retroﬁt analysis under uncertainty. Energy\n",
      "Build. 2012 ,47, 550–560. [CrossRef]\n",
      "8. Qiu, S.; Li, Z.; Li, Z.; Li, J.; Long, S.; Li, X. Model-free control method based on reinforcement learning for building cooling water\n",
      "systems: Validation by measured data-based simulation. Energy Build. 2020 ,218, 110055. [CrossRef]\n",
      "9. Claessens, B.J.; Vanhoudt, D.; Desmedt, J.; Ruelens, F. Model-free control of thermostatically controlled loads connected to a\n",
      "district heating network. Energy Build. 2018 ,159, 1–10. [CrossRef]\n",
      "10. Lork, C.; Li, W.T.; Qin, Y.; Zhou, Y.; Yuen, C.; Tushar, W.; Saha, T.K. An uncertainty-aware deep reinforcement learning framework\n",
      "for residential air conditioning energy management. Appl. Energy 2020 ,276, 115426. [CrossRef]\n",
      "11. Ahn, K.U.; Park, C.S. Application of deep Q-networks for model-free optimal control balancing between different HVAC systems.\n",
      "Sci. Technol. Built Environ. 2020 ,26, 61–74. [CrossRef]\n",
      "12. Brandi, S.; Piscitelli, M.S.; Martellacci, M.; Capozzoli, A. Deep reinforcement learning to optimise indoor temperature control and\n",
      "heating energy consumption in buildings. Energy Build. 2020 ,224, 110225. [CrossRef]\n",
      "13. Du, Y.; Zandi, H.; Kotevska, O.; Kurte, K.; Munk, J.; Amasyali, K.; Mckee, E.; Li, F. Intelligent multi-zone residential HVAC control\n",
      "strategy based on deep reinforcement learning. Appl. Energy 2021 ,281, 116117. [CrossRef]\n",
      "14. Ding, Z.; Fu, Q.; Chen, J.; Wu, H.; Lu, Y.; Hu, F. Energy-efﬁcient control of thermal comfort in multi-zone residential HVAC via\n",
      "reinforcement learning. Connect. Sci. 2022 ,34, 2364–2394. [CrossRef]\n",
      "15. Qiu, S.; Li, Z.; Li, Z.; Wu, Q. Comparative Evaluation of Different Multi-Agent Reinforcement Learning Mechanisms in Condenser\n",
      "Water System Control. Buildings 2022 ,12, 1092. [CrossRef]\n",
      "16. Amasyali, K.; Munk, J.; Kurte, K.; Kuruganti, T.; Zandi, H. Deep reinforcement learning for autonomous water heater control.\n",
      "Buildings 2021 ,11, 548. [CrossRef]\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"16. Amasyali, K.; Munk, J.; Kurte, K.; Kuruganti, T.; Zandi, H. Deep reinforcement learning for autonomous water heater control.\n",
      "Buildings 2021 ,11, 548. [CrossRef]\n",
      "17. Li, B.; Xia, L. A multi-grid reinforcement learning method for energy conservation and comfort of HVAC in buildings. In\n",
      "Proceedings of the 2015 IEEE International Conference on Automation Science and Engineering (CASE), Gothenburg, Sweden,\n",
      "24–28 August 2015; pp. 444–449. [CrossRef]\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Buildings 2022 ,12, 1787 22 of 22\n",
      "18. Yu, Z.; Yang, X.; Gao, F.; Huang, J.; Tu, R.; Cui, J. A Knowledge-based reinforcement learning control approach using deep Q\n",
      "network for cooling tower in HVAC systems. In Proceedings of the 2020 Chinese Automation Congress, CAC 2020, Shanghai,\n",
      "China, 6–8 November 2020; pp. 1721–1726. [CrossRef]\n",
      "19. Fu, Q.; Chen, X.; Ma, S.; Fang, N.; Xing, B.; Chen, J. Optimal control method of HVAC based on multi-agent deep reinforcement\n",
      "learning. Energy Build. 2022 ,270, 112284. [CrossRef]\n",
      "20. Yang, L.; Nagy, Z.; Gofﬁn, P .; Schlueter, A. Reinforcement learning for optimal control of low exergy buildings. Appl. Energy 2015 ,\n",
      "156, 577–586. [CrossRef]\n",
      "21. Sutton, R.; Barto, A. Reinforcement Learning: An Introduction , 2nd ed.; MIT Press: Cambridge, MA, USA; London, UK, 2018.\n",
      "22. Zhou, Z.H.; Feng, J. Deep forest. Natl. Sci. Rev. 2019 ,6, 74–86. [CrossRef]\n",
      "23. Mnih, V .; Kavukcuoglu, K.; Silver, D.; Rusu, A.A.; Veness, J.; Bellemare, M.G.; Graves, A.; Riedmiller, M.; Fidjeland, A.K.;\n",
      "Ostrovski, G.; et al. Human-level control through deep reinforcement learning. Nature 2015 ,518, 529–533. [CrossRef]\n",
      "24. Fu, Q.; Han, Z.; Chen, J.; Lu, Y.; Wu, H.; Wang, Y. Applications of reinforcement learning for building energy efﬁciency control: A\n",
      "review. J. Build. Eng. 2022 ,50, 104165. [CrossRef]\n",
      "25. Fu, Q.; Li, K.; Chen, J.; Wang, J. A Novel Deep-forest-based DQN method for Building Energy Consumption Prediction. Buildings\n",
      "2022 ,12, 131. [CrossRef]\n",
      "26. Li, Z.; Huang, G.; Sun, Y. Stochastic chiller sequencing control. Energy Build. 2014 ,84, 203–213. [CrossRef]\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" This article discusses the use of Deep Forest-Based Deep Q-Network (DF-DQN) to optimize the running frequency of the cooling water pump and cooling tower in the cooling water system of HVAC systems in order to reduce energy consumption. The results of the experiment show that DF-DQN can realize energy savings from the first year, while DQN realized savings from the third year. DF-DQN is more suitable for engineering practice than traditional RL methods due to its ability to avoid unnecessary waste caused by exploration in the early stage.\n",
      "\n",
      " The optimal control of the cooling water system in HVAC systems is essential for reducing energy consumption and operation costs, as well as ensuring the thermal comfort of occupants. Optimal control policies can be classified into model-based and model-free methods.\n",
      "\n",
      " This paper explores the use of DF-DQN to control a cooling water system in HVAC and to realize energy savings from the early stage. The introduction of DF maps the original DQN output action space into a new smaller action space, which could accelerate the convergence speed of DQN. The experimental results show that DF-DQN can save energy from the first year, while DQN can achieve similar energy saving from the third year.\n",
      "\n",
      " This paper presents DF-DQN, a deep reinforcement learning algorithm for energy saving control. The performance of DF-DQN was verified in an environment based on a real cooling water system. The data used for DF-DQN and other compared methods were collected from a real-world system, and the simulation environment was built based on this system. The code and experimental data are available online.\n",
      "\n",
      "\n",
      "\n",
      "Recent research has applied Reinforcement Learning (RL) methods to HVAC systems in order to achieve energy savings and comfort. However, RL algorithms can take a long time to converge, leading to unnecessary energy wastage and cost increases. To avoid this, researchers have tried to speed up the convergence of RL algorithms. Applications of RL in HVAC have been explored, such as using Q-learning and DQN to control the cooling water system and temperature set point of the heating system terminal unit. Additionally, researchers have tried to improve the convergence speed of RL algorithms by using multi-grid Q-learning, exploration policies, and multi-agent RL.\n",
      "\n",
      "\n",
      "This article explores the potential of Reinforcement Learning (RL) as a control technique in HVAC systems. RL is a type of sequential decision-making, where the agent learns by interacting with the environment, as illustrated in Figure 1.\n",
      "\n",
      " Reinforcement Learning (RL) is a classical formalization of sequential decision-making used to model problems. It is used to maximize a cumulative numerical reward by selecting an action from a policy and receiving a scalar reward from the environment. Deep Forest (DF) is a decision tree ensemble approach used for classification tasks that can obtain good performance with different data.\n",
      "\n",
      " Deep Forest (DF) is a decision tree ensemble approach used for classification tasks. It obtains good performance in most cases due to two techniques: multi-grained scanning and the cascade forest structure. Multi-grained scanning uses sliding windows of various sizes to obtain more feature sub-samples, while the cascade forest structure enhances the representation learning ability of DF.\n",
      "\n",
      " This paper discusses the use of Deep Forest (DF) and Deep Q-Network (DQN) to control a cooling water system in order to reduce energy consumption. DF is a cascade forest structure which takes a feature vector as input and outputs an aggregate value. DQN is a method proposed by Google's DeepMind which can solve problems with large or continuous state spaces. The cooling water system platform contains four chillers, three cooling water pumps, and seven cooling towers.\n",
      "\n",
      " DQN, a method proposed by Google's DeepMind in 2015, has been applied in HVAC controls in recent years. It is beneficial due to its two specific techniques: experience replay and two networks (Q-network and target network). This paper focuses on controlling a cooling water system to reduce energy consumption, which includes four chillers, three cooling water pumps, and seven cooling towers.\n",
      "\n",
      " This article discusses the layout of a cooling water system and the system coefficient of performance (COP) used to measure the energy-saving performance of HVAC systems. The system COP is defined in Equation (5) and the system cooling load is defined in Equation (6). To simulate the system, some real data and parameters were collected, and the regression method was used to attain the chiller's COP and power. Other related parameters are shown in Equations (9)-(11).\n",
      "\n",
      " This article discusses the development of a controller for a cooling system. It outlines the models used to calculate the power of the cooling water pump, the cooling tower, and the controller. The accuracy of the models was evaluated using MAPE and CVRMSE, and the results showed that the accuracy was within an acceptable range. The data collected from an actual system was used to verify the proposed method.\n",
      "\n",
      " This paper presents a simulation process to verify a proposed method. Data was collected from an actual system from July 1, 2021 to October 10, 2021 with a sample interval of half an hour. Figures 5 and 6 show the temporal distribution of the cooling load and wet-bulb temperature, respectively. The deeper the color, the heavier the load.\n",
      "\n",
      " This paper presents a method for controlling a cooling water system using reinforcement learning (RL) methods. The environment is modeled using a Markov Decision Process (MDP) with the combination of ambient wet-bulb temperature and system cooling load as the state, operating frequencies of cooling tower fans and cooling water pumps as the action, and COP as the reward. The overall framework of DF-DQN for control is depicted in Figure 7.\n",
      "\n",
      " This paper presents a deep forest classification model to obtain the relationship between the actual frequency of the equipment operation and the base number under some states. The labeled state data were used to train the model, of which 80% was used for training and 20% was used to test the accuracy of the trained model. After training, the model can output a label for the new state, which can be converted into a sign to shrink the action space. The difference between the operating frequency of the actual equipment and the base number is also needed.\n",
      "\n",
      "\n",
      "This paper presents a method of reducing the action space of a problem with large action space or multiple action combinations by introducing a DF classifier. The DF classifier labels each state and replaces the original action space with a smaller action space combined with the label. The introduction of DF in DQN reduces the original action space of each equipment from 31 to 16, and the combined action of the two equipment reduces the action space by nearly 3/4 with the increase in equipment types. Theoretically, if DF can divide each action into M categories with the same number, and the final combined actions include N kinds, the reduced action space of DF-DQN can follow Equation (19).\n",
      "\n",
      " This paper presents a DF-DQN algorithm for a cooling water system. The algorithm is based on Equation (19), which reduces the complexity of the problem by shrinking the original action space of (1M)N to 1/4 of its original size. The details of the DF-DQN algorithm are outlined in Algorithm 1.\n",
      "\n",
      " This paper presents a DF-DQN algorithm for controlling a cooling water system. The algorithm is initialized with a replay memory and action value function, and outliers in the training set are detected and replaced. The training set is split into 80% for training and 20% for testing, and the deep forest classifier is trained. The algorithm is compared to three other benchmark methods: DQN, baseline control, and model-based control. Parameters are set and experiments are conducted to verify the performance of DF-DQN.\n",
      "\n",
      " This paper explores the use of Deep Q-Learning (DQN) and Decision Forests (DF) to control cooling water pumps and cooling towers in a simulated environment. The agent in DF-DQN takes an ε-greedy policy to select the action, and parameters such as γ, memory capacity, and learning rate are set to optimize the agent's performance. The accuracy of DF affects the performance of DF-DQN, and experiments show that DF-DQN has better energy-saving performance than DQN.\n",
      "\n",
      " This study compared the performance of DF-DQN (false label) and DF-DQN in a simulation environment. The convergence of the two methods was shown in Figure 10, where one episode in the training process is one year. The accuracy of DF affected the performance of DF-DQN, and the cumulative reward of DF-DQN was higher than that of DF-DQN (false label). The COP of DF-DQN (false label) was lower than that of DF-DQN in 20 years, and the energy-saving effect decreased accordingly. The partial energy-saving effect comparison result can be found in Table 2.\n",
      "\n",
      " This article compares the energy-saving effect of two methods, DF-DQN and DF-DQN (false label), using the baseline control method of the system as a benchmark. The comparison results show that DF-DQN (false label) was worse than DF-DQN, and the direct reason for this result was the wrong labels. The comparison also found that the accuracy of DF directly affected the performance of DF-DQN, and the low accuracy of DF led to a decrease in the performance of DF-DQN. DF-DQN and DQN both converged at last, but in the beginning episodes, DF-DQN achieved a higher cumulative reward.\n",
      "\n",
      " This paper compares the performance of DF-DQN, DQN, baseline control, and model-based control in terms of COP, cumulative power, and energy-saving effect. The results show that DF-DQN outperformed DQN in the early stages, but DQN eventually outperformed DF-DQN due to the accuracy of DF. The model-based control method was the best method in terms of COP, while the baseline control method was the worst. The COP of DQN and DF-DQN gradually increased, indicating their energy-saving performance was becoming better.\n",
      "\n",
      "\n",
      "\n",
      "This study compared the performance of two control methods, DQN and DF-DQN, over a 20 year period. The results showed that DF-DQN had a better control effect from the beginning and was more stable than DQN over the 20 year period. Additionally, the annual cumulative power under the two methods' control policies was compared, with the results shown in Figure 15.\n",
      "\n",
      " This study compared the energy-saving performance of four methods: DQN, DF-DQN, model-based control, and baseline control. The results showed that DF-DQN had the best energy-saving performance, with a higher COP and lower cumulative power than the other methods. DQN had a lower COP than the baseline control method in the first year, but its COP gradually increased and its cumulative power decreased until the fourth year. DF-DQN had a lower cumulative power than the baseline control method from the first year and its energy-saving effect was better than DQN in the early stage.\n",
      "\n",
      " This study compared the energy-saving effects of four different methods, with the baseline control method as the benchmark. Results showed that the model-based control method had the lowest cumulative power, while the baseline control method had a relatively poor energy-saving effect. DQN's cumulative power was larger than the model-based control method and less than the baseline control method from the third year. DQN's cumulative power decreased until the fourth year, and then remained stable.\n",
      "\n",
      " This paper explored the energy-saving effects of three methods: baseline control, DQN, and DF-DQN. The results showed that the model-based control method had the best energy-saving effect, reaching 13.775%. DQN could not achieve the goal of energy saving until the third year, but eventually reached an average energy-saving effect of 7.972%. DF-DQN could achieve the goal of energy saving from the first year, and had an average energy-saving effect of 11.035%. The results showed that DF-DQN may have a better energy-saving effect than DQN in general.\n",
      "\n",
      " This paper examines the performance of a deep forest classifier-based deep Q-learning (DF-DQN) control system for water system control in HVAC compared to DQN, baseline control method, and model-based control method. The results show that DF-DQN converges faster than DQN and has good energy-saving effects in the early stage, but its performance is affected by the accuracy of the DF classifier. The authors suggest future work to improve the accuracy of the DF classifier and to use multi-agent reinforcement learning when more equipment is involved.\n",
      "\n",
      " This paper presents a system for energy-saving control using a Deep Q-Network (DQN) and a Double-Frequency Deep Q-Network (DF-DQN). The system works by observing the state of the system and deciding to turn it on or off according to the cooling load (CL). The DF-DQN controller is used to control cooling water pumps and cooling towers, select the frequency of them, and combine them into an action. The energy-saving effects obtained by all methods compared to baseline control are shown in Table A1, with the DF-DQN method showing the highest energy-saving effect.\n",
      "\n",
      " This article compares the energy saving performance of four different control strategies for HVAC systems in buildings: DQN, DF-DQN, DF-DQN (False Label), and Model-Based Control. The results show that Model-Based Control has the highest energy saving performance, followed by DQN and DF-DQN. DF-DQN (False Label) had the lowest energy saving performance. The article also discusses various data mining and reinforcement learning approaches for improving the optimal control of HVAC systems.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" This paper presents two studies on the use of deep reinforcement learning for autonomous control of water heaters and HVAC systems in buildings. The first study by Amasyali et al. (2021) focuses on the use of deep reinforcement learning for autonomous water heater control. The second study by Li and Xia (2015) focuses on the use of a multi-grid reinforcement learning method for energy conservation and comfort of HVAC in buildings.\n",
      "\n",
      " This article discusses the use of knowledge-based reinforcement learning control approaches, such as deep Q networks, multi-agent deep reinforcement learning, and deep forest, for HVAC systems and building energy efficiency control. It also reviews the literature on reinforcement learning for building energy efficiency control, building energy consumption prediction, and stochastic chiller sequencing control.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" This paper explores the use of Deep Forest-Based Deep Q-Network (DF-DQN) to control a cooling water system in HVAC and to realize energy savings from the early stage. The introduction of DF maps the original DQN output action space into a new smaller action space, which could accelerate the convergence speed of DQN. The experimental results show that DF-DQN can save energy from the first year, while DQN can achieve similar energy saving from the third year. The comparison results show that DF-DQN had the best energy-saving performance, with a higher COP and lower cumulative power than the other methods.\n",
      "\n",
      " This paper reviews the use of deep reinforcement learning for autonomous control of water heaters and HVAC systems in buildings. It focuses on two studies, one by Amasyali et al. (2021) on water heater control and one by Li and Xia (2015) on energy conservation and comfort of HVAC in buildings. It also reviews the literature on reinforcement learning for building energy efficiency control, building energy consumption prediction, and stochastic chiller sequencing control.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " This paper reviews the use of deep reinforcement learning for autonomous control of water heaters\n",
      "and HVAC systems in buildings. It compares the performance of Deep Forest-Based Deep Q-Network (DF-\n",
      "DQN) to other methods, and finds that DF-DQN can save energy from the first year, while other\n",
      "methods take longer to achieve similar energy savings. It also reviews the literature on\n",
      "reinforcement learning for building energy efficiency control, building energy consumption\n",
      "prediction, and stochastic chiller sequencing control.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"map_reduce\",\n",
    "                             verbose=True\n",
    "                             )\n",
    "\n",
    "\n",
    "output_summary = chain.run(pages)\n",
    "wrapped_text = textwrap.fill(output_summary, \n",
    "                             width=100,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d61747be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Deep Forest-Based DQN (DF-DQN) proposed for energy saving control in HVAC cooling water systems\n",
      "•\n",
      "DF-DQN converges faster and has better energy-saving effect than DQN in early stage\n",
      "• Experiments\n",
      "show DF-DQN can realize energy savings from first year, while DQN from third year\n",
      "• DF-DQN can\n",
      "improve energy-saving effect by 11.035% on average every year\n",
      "• Data mining approaches and adaptive-\n",
      "predictive control strategies developed for HVAC systems in smart buildings\n",
      "• Model-free control\n",
      "methods based on reinforcement learning applied to building cooling water systems and district\n",
      "heating networks\n",
      "• Deep Q-networks applied for model-free optimal control balancing between\n",
      "different HVAC systems\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, \n",
    "                        input_variables=[\"text\"])\n",
    "\n",
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"map_reduce\",\n",
    "                             map_prompt=BULLET_POINT_PROMPT, \n",
    "                             combine_prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "# chain.llm_chain.prompt= BULLET_POINT_PROMPT\n",
    "# chain.combine_document_chain.llm_chain.prompt= BULLET_POINT_PROMPT\n",
    "\n",
    "output_summary = chain.run(pages)\n",
    "wrapped_text = textwrap.fill(output_summary, \n",
    "                             width=100,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81b970c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  This article discusses the use of Deep Forest-Based Deep Q-Network (DF-DQN) to optimize the\n",
      "running frequency of the cooling water pump and cooling tower in the cooling water system of HVAC\n",
      "systems in order to reduce energy consumption. Reinforcement learning (RL), a data-driven and model-\n",
      "free method in artiﬁcial intelligence, is used to tackle this problem. DF-DQN maps the original\n",
      "action space to a smaller one and combines the label of Deep Forest (DF), a decision tree ensemble\n",
      "approach used for classification tasks, to attain the ﬁnal control action. Markov's decision process\n",
      "(MDP) is used to model RL problems, and the cascade forest structure is used to enhance the\n",
      "representation learning ability of DF, while the training process of the deep forest is efficient,\n",
      "even with small training data scales. DQN, a method proposed by Google’s DeepMind in 2015, is used\n",
      "to solve problems with large or continuous state space, and utilizes the mechanism of experience\n",
      "replay to eliminate the correlation of network inputs. The state data, including cooling load and\n",
      "wet-bulb temperature, is labeled and the action is discretized and limited within a reasonable\n",
      "range. COP is taken\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "\n",
    "output_summary = chain.run(pages)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64d56ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This article presents a Deep Forest-Based Deep Q-Network (DF-DQN) for energy saving control in HVAC\n",
      "cooling water systems. Optimal control of the cooling water system is crucial for reducing energy\n",
      "consumption of the HVAC system and ensuring the thermal comfort of occupants. The DF-DQN uses\n",
      "historical data and expert experience to train a deep forest classifier, which maps the original\n",
      "action space of DQN to a smaller one, allowing for faster convergence and better energy-saving\n",
      "effects than DQN in the early stage. The deep forest classifier uses multi-grained scanning and a\n",
      "cascade forest structure to obtain more feature sub-samples and enhance the representation learning\n",
      "ability of DF. The agent in DF-DQN took the ε−greedy policy to select the action and the parameters\n",
      "were set to ensure the agent could explore the environment as much as possible. The workflow of this\n",
      "system is shown in Appendix A and the experimental results showed that the accuracy of DF can reach\n",
      "97.319% and 99.694%. The reward was defined by Equation (16), namely COP, and the higher reward not\n",
      "only conveyed that it had better converge, but also represented that the method had better energy-\n",
      "saving effects compared to\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following extracting the key information:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, \n",
    "                        input_variables=[\"text\"])\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")\n",
    "\n",
    "chain = load_summarize_chain(OpenAI(temperature=0), \n",
    "                             chain_type=\"refine\", \n",
    "                             return_intermediate_steps=True, \n",
    "                             question_prompt=PROMPT, \n",
    "                             refine_prompt=refine_prompt)\n",
    "\n",
    "output_summary = chain({\"input_documents\": pages}, return_only_outputs=True)\n",
    "wrapped_text = textwrap.fill(output_summary['output_text'], \n",
    "                             width=100,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54902cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
