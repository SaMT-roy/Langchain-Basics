{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc996925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Text Splitting & Docloader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] =  ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52995a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9d2ee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 646/646 [00:17<00:00, 36.26it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader('singapore_text/Textfiles3/English/', glob=\"*.txt\", show_progress=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6e2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c6952e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = FAISS.from_documents(texts,\n",
    "                                 bge_embeddings\n",
    "                                #  OpenAIEmbeddings()\n",
    "                                 ).as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fe82ba",
   "metadata": {},
   "source": [
    "# Adding contextual compression with an LLMChainExtractor\n",
    "Now let's wrap our base retriever with a ContextualCompressionRetriever. We'll add an LLMChainExtractor, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74706946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# making the compressor\n",
    "llm = GooglePalm(temperature=0)\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# it needs a base retriever (we're using FAISS Retriever) and a compressor (Made above)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor,\n",
    "                                                       base_retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d45a7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], output_parser=NoOutputParser(), template='Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return NO_OUTPUT. \\n\\nRemember, *DO NOT* edit the extracted parts of the context.\\n\\n> Question: {question}\\n> Context:\\n>>>\\n{context}\\n>>>\\nExtracted relevant parts:')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compressor prompt\n",
    "compressor.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28bde453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saptarshimallikthakur/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/saptarshimallikthakur/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/saptarshimallikthakur/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/saptarshimallikthakur/miniconda3/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore is a modern, urban, clean and very western influenced city.\n"
     ]
    }
   ],
   "source": [
    "compressed_docs = compression_retriever.get_relevant_documents(\"What is singapore?\")\n",
    "print(compressed_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15879d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
